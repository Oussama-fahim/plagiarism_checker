{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_parse import LlamaParse\n",
    "from llama_parse.base import ResultType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from llama_cloud_services.parse.utils import Language\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-a2C7FgYfP1hzX3pXuvtdaNmexAqsuRnJIJ2G6MjbBrfuS3QY\"  # Remplace par ta clé API\n",
    "\n",
    "parser_fr = LlamaParse(result_type=ResultType.MD, language=Language.FRENCH)\n",
    "parser_en = LlamaParse(result_type=ResultType.MD, language=Language.ENGLISH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de philosophie.pdf...\n",
      "Started parsing the file under job_id bcb1ab79-e166-4be6-918c-19ee9e70e745\n",
      ".✅ Tous les textes ont été extraits et sauvegardés dans : plagia_data.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pdf_files = [\n",
    "    (\"philosophie.pdf\", parser_fr)\n",
    "   \n",
    "    \n",
    "    # ajoute autant que nécessaire\n",
    "]\n",
    "\n",
    "# Nom du fichier Markdown de sortie\n",
    "output_filename = \"plagia_data.md\"\n",
    "\n",
    "# Traitement et sauvegarde\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    for file_name, parser in pdf_files:\n",
    "        print(f\"Traitement de {file_name}...\")\n",
    "        documents = parser.load_data(file_name)\n",
    "        f.write(f\"# Contenu extrait de : {file_name}\\n\\n\")\n",
    "        for doc in documents:\n",
    "            f.write(doc.text + \"\\n\\n\")\n",
    "\n",
    "print(f\"✅ Tous les textes ont été extraits et sauvegardés dans : {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiba\\AppData\\Local\\Temp\\ipykernel_10004\\1223247682.py:12: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opération terminée avec succès:\n",
      "- 914 paragraphes traités\n",
      "- Base vectorielle sauvegardée dans: philo_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiba\\AppData\\Local\\Temp\\ipykernel_10004\\1223247682.py:24: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vecdb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 1. Lecture du fichier Markdown\n",
    "with open(\"plagia_data.md\", encoding='utf-8') as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "# 2. Découpage par paragraphes (2 sauts de ligne ou plus)\n",
    "paragraphs = [p.strip() for p in markdown_content.split('\\n\\n') if p.strip()]\n",
    "\n",
    "# 3. Création des documents\n",
    "documents = [Document(page_content=paragraph) for paragraph in paragraphs]\n",
    "\n",
    "# 4. Initialisation des embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "\n",
    "# 5. Configuration de la base vectorielle\n",
    "persist_directory = \"philo_db\"\n",
    "vecdb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"rag-chroma\"\n",
    ")\n",
    "\n",
    "# 6. Persistance des données\n",
    "vecdb.persist()\n",
    "print(\"Opération terminée avec succès:\")\n",
    "print(f\"- {len(documents)} paragraphes traités\")\n",
    "print(f\"- Base vectorielle sauvegardée dans: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
