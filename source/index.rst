================================================
Documentation du projet IA de plagiarism checker
================================================

Bienvenue dans la documentation du projet D√©tection de plagiat par Intelligence Artificielle √† l'aide de RAG et d'analyse s√©mantique avanc√©e.Ce document d√©taille les m√©thodologies, les outils utilis√©s, ainsi que les r√©sultats obtenus pour identifier les cas de plagiat, qu‚Äôils soient exacts, paraphras√©s ou s√©mantiquement similaires.L‚Äôobjectif de ce projet est de d√©velopper un syst√®me de d√©tection robuste en combinant plusieurs approches

Pr√©par√© par :
-------------
  **-Oussama Fahim**

  **-Fatima el Fadili**

Encad√© par :
------------
    **- M.hajji**

Table des mati√®res
------------------

- `Introduction <index.html#id1>`_
- `Objectifs du projet <index.html#id2>`_
- `Installation <index.html#id3>`_
- `Pipeline <index.html#id4>`_
- `Cr√©ation d'une Base de Donn√©es Vectorielle <index.html#id5>`_
- `Application des Approches de Recherche Hybride <index.html#id6>`_
- `Cr√©ation d'une interface streamlit <index.html#id7>`_ 
- `R√©sultats <index.html#id8>`_
- `Travaux Futurs <index.html#id9>`_
- `Conclusion <index.html#id10>`_

Introduction
============

La d√©tection automatique du plagiat est devenue un enjeu crucial dans le monde acad√©mique et professionnel, o√π la v√©rification de l'originalit√© d'un contenu est essentielle √† la cr√©dibilit√© intellectuelle et √† l‚Äôint√©grit√© des travaux. 

Ce projet propose une solution intelligente et robuste de d√©tection de plagiat en temps r√©el, exploitant les derni√®res avanc√©es en **traitement du langage naturel (NLP)**, **recherche vectorielle**, et **mod√®les de similarit√© s√©mantique multilingue**.

En s‚Äôappuyant sur une combinaison d‚Äô**embeddings puissants**, de **moteurs de recherche hybride**, de **mod√®les cross-encoders**, ainsi que d‚Äôune **analyse stylistique avanc√©e**, cette plateforme permet de d√©tecter diff√©rents types de plagiat : copie exacte, paraphrase, similitude conceptuelle, ou correspondance multilingue.

Le tout est encapsul√© dans une interface interactive d√©velopp√©e avec **Streamlit**, offrant √† l'utilisateur une exp√©rience fluide, visuelle, et totalement explicable.

Objectifs du projet
===================

- **Fournir un syst√®me de d√©tection de plagiat automatis√© et intelligent** :
  
  - Bas√© sur une base vectorielle *Chroma* enrichie par des embeddings (*Ollama embeddings*).
  - Capable d‚Äôidentifier non seulement des copies exactes, mais aussi des paraphrases et similitudes conceptuelles, y compris entre langues diff√©rentes (fran√ßais/anglais).

- **D√©montrer l‚Äôutilisation combin√©e de technologies modernes** :
  
  - üß† *LangChain*, *TF-IDF*, *Cross-Encoder* (MS-MARCO) pour l‚Äôanalyse s√©mantique.
  - üìö *SpaCy* et *textstat* pour l‚Äôanalyse stylistique, lisibilit√©, structure grammaticale, diversit√© lexicale.
  - üìä *Streamlit*, *Plotly*, *WordCloud*, *Pyvis* pour la visualisation avanc√©e et l‚Äôexplicabilit√©.

- **Assurer une analyse compl√®te et interpr√©table** :
  
  - G√©n√©ration d‚Äôun rapport technique JSON d√©taill√©.
  - R√©sum√© ex√©cutif avec score global, alertes de plagiat, et recommandations.
  - Visualisation des correspondances (r√©seaux interactifs, nuages de mots, graphiques).

- **Garantir une exp√©rience utilisateur professionnelle** :
  
  - Interface responsive avec personnalisation CSS.
  - Chargement de texte via saisie directe, fichiers (.pdf, .docx‚Ä¶), ou URL.
  - T√©l√©chargement dynamique de rapports avec score et diagnostic.




Installation
============

Les biblioth√®ques suivantes sont n√©cessaires pour le projet :

1. **os** : Manipulation des fichiers et chemins.
2. **pickle** : S√©rialisation et sauvegarde des objets Python.
3. **cv2** : Traitement d'images avec OpenCV.
4. **numpy** : Manipulation de matrices et calculs num√©riques.
5. **streamlit** : Interface web interactive pour l‚Äôutilisateur.
6. **langchain.vectorstores** : Gestion des bases vectorielles avec Chroma.
7. **langchain_community.embeddings** : G√©n√©ration des embeddings avec Ollama.
8. **langdetect** : D√©tection automatique de la langue d‚Äôun texte.
9. **typing** : Annotations de types (`List`, `Dict`, etc.).
10. **time** : Gestion du temps (dur√©e d‚Äôanalyse, timestamps).
11. **pandas** : Manipulation et affichage de tableaux de donn√©es.
12. **difflib** : Comparaison de cha√Ænes pour la similarit√© exacte.
13. **matplotlib.pyplot** : Visualisation de donn√©es.
14. **plotly.express** : Graphiques interactifs (camemberts, barres).
15. **collections.defaultdict** : Regroupement d‚Äô√©l√©ments similaires.
16. **re** : Expressions r√©guli√®res pour le nettoyage de texte.
17. **hashlib** : Hachage des textes pour la d√©tection exacte.
18. **sentence_transformers** : Calcul avanc√© de similarit√© s√©mantique (CrossEncoder).
19. **sklearn.feature_extraction.text** : TF-IDF vectorisation.
20. **sklearn.metrics.pairwise** : Similarit√© cosinus.
21. **PIL.Image** : Chargement et affichage d‚Äôimages.
22. **requests** : Requ√™te HTTP pour charger des images ou contenus.
23. **io.BytesIO** : Manipulation de contenu binaire.
24. **json** : S√©rialisation JSON pour les rapports.
25. **networkx** : Cr√©ation de graphes de similarit√©.
26. **pyvis.network** : Visualisation interactive de r√©seaux.
27. **tempfile** : Cr√©ation de fichiers temporaires.
28. **spacy** : Analyse grammaticale et entit√©s nomm√©es.
29. **wordcloud.WordCloud** : Nuage de mots bas√© sur les correspondances.
30. **textstat** : Analyse de lisibilit√©.
31. **streamlit.components.v1.html** : Affichage de composants HTML personnalis√©s.
32. **docx2txt** : Extraction de texte depuis fichiers Word.
33. **PyPDF2** : Extraction de texte depuis fichiers PDF.
34. **base64** : Encodage d‚Äôimages pour l‚Äôaffichage CSS.
35. **annotated_text** : Mise en √©vidence de texte dans Streamlit.
36. **st_aggrid** : Tableaux interactifs dans Streamlit.
37. **ollama** : Requ√™tes vers un mod√®le de langage local.

.. code-block:: python

   import os
   import pickle
   import cv2
   import numpy as np
   import streamlit as st
   import time
   import pandas as pd
   import matplotlib.pyplot as plt
   import plotly.express as px
   import re
   import hashlib
   import json
   import tempfile
   import requests
   import base64
   import docx2txt
   import PyPDF2
   import networkx as nx
   from PIL import Image
   from io import BytesIO
   from difflib import SequenceMatcher
   from collections import defaultdict
   from typing import List, Dict, Any, Tuple
   from sentence_transformers import CrossEncoder
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.metrics.pairwise import cosine_similarity
   from langchain.vectorstores import Chroma
   from langchain_community.embeddings import OllamaEmbeddings
   from wordcloud import WordCloud
   import spacy
   import textstat
   from streamlit.components.v1 import html
   from annotated_text import annotated_text
   from st_aggrid import AgGrid
   import ollama


pipeline
========

.. list-table::
   :widths: 200 200
   :align: center

   * - .. image:: image/1.png
         :alt: PIPLINE 
         :width: 700px
     - .. image:: image/2.png
         :alt: Image 2
         :width: 700px

**explication de pipline:**

*Phase 1: Pr√©paration de la Base Vectorielle*

.. list-table::
   :header-rows: 1
   :widths: 10 30 60

   * - √âtape
     - Outils/M√©thodes
     - Description
   * - 1. Extraction
     - LlamaParse (FR/EN)
     - Conversion des PDF/DOCX en Markdown propre
   * - 2. Nettoyage
     - Regex + Unicode Normalization
     - Suppression des en-t√™tes, pieds de page, caract√®res sp√©ciaux
   * - 3. D√©coupage
     - ``split('\\n\\n')``
     - S√©paration en paragraphes (1 paragraphe = 1 document)
   * - 4. Embeddings
     - OllamaEmbeddings (mxbai-embed-large)
     - Vectorisation des paragraphes
   * - 5. Stockage
     - Chroma DB
     - Indexation avec m√©tadonn√©es (source, langue)
   * - 6. Persistance
     - ``vecdb.persist()``
     - Sauvegarde locale dans ``philo_db``

*Phase 2: Analyse de Plagiat (Frontend/Backend)*

.. list-table::
   :header-rows: 1
   :widths: 10 30 60

   * - √âtape
     - Outils/M√©thodes
     - Description
   * - 1. Input Utilisateur
     - Streamlit (``file_uploader``/``text_area``)
     - Support pour texte direct, fichiers, ou URLs
   * - 2. Pr√©-processing
     - ``langdetect`` + ``spacy``
     - D√©tection de langue et nettoyage
   * - 3. Recherche Hybride
     - Combinaison de 3 m√©thodes:
     - 
   * - 
     - ‚Ä¢ **Exact Match** (MD5 + ``SequenceMatcher``)
     - D√©tection de copies mot-√†-mot
   * - 
     - ‚Ä¢ **Semantic Search** (Ollama + Cross-Encoder)
     - Similarit√© conceptuelle (seuil: 0.4-1.0)
   * - 
     - ‚Ä¢ **Multilingue** (Traduction via Llama3)
     - Comparaison FR‚ÜîEN
   * - 4. Post-Traitement
     - ``networkx`` + ``pyvis``
     - G√©n√©ration du r√©seau de similarit√©
   * - 5. Rapport
     - JSON + Streamlit
     - Export des r√©sultats d√©taill√©s


Cr√©ation d'une Base de Donn√©es Vectorielle 
==========================================

Ce guide fournit une proc√©dure compl√®te pour transformer un ou plusieurs fichiers PDF en une base de donn√©es vectorielle, utilisable notamment pour la d√©tection de similarit√© textuelle ou de plagiat. L'approche repose sur l'utilisation combin√©e de **LlamaParse** pour l'extraction intelligente de texte structur√© et de **LangChain** pour la vectorisation et la gestion des documents.

.. contents:: Sommaire
   :depth: 2
   :local:

**√âtape 1 : Installation des D√©pendances**


Cette premi√®re √©tape consiste √† importer l'ensemble des librairies n√©cessaires au bon fonctionnement du pipeline. 

.. code-block:: python
   :linenos:

   import os
   from llama_parse import LlamaParse
   from llama_parse.base import ResultType
   from langchain.text_splitter import RecursiveCharacterTextSplitter
   from langchain.vectorstores import Chroma
   from langchain.embeddings import HuggingFaceEmbeddings
   from langchain_core.documents import Document
   from llama_cloud_services.parse.utils import Language
   from langchain_community.embeddings.ollama import OllamaEmbeddings

Les modules import√©s remplissent des r√¥les sp√©cifiques :
- `llama_parse` permet d'extraire le contenu structur√© des PDF (en Markdown ici).
- `langchain` permet de g√©rer la transformation du texte en vecteurs ainsi que leur stockage dans une base.
- `OllamaEmbeddings` fournit un mod√®le d'embedding performant pour convertir du texte en vecteurs num√©riques.

**√âtape 2 : Configuration de l'API LlamaParse**

Avant de lancer l'extraction, il est n√©cessaire de configurer LlamaParse avec une cl√© API valide. On peut √©galement sp√©cifier la langue du document pour am√©liorer la pr√©cision de l‚Äôanalyse.

.. code-block:: python
   :linenos:

   os.environ["LLAMA_CLOUD_API_KEY"] = "llx-a2C7FgYfP1hzX3pXuvtdaNmexAqsuRnJIJ2G6MjbBrfuS3QY"
   
   parser_fr = LlamaParse(
       result_type=ResultType.MD, 
       language=Language.FRENCH
   )
   parser_en = LlamaParse(
       result_type=ResultType.MD,
       language=Language.ENGLISH
   )

Deux parseurs sont initialis√©s ici : un pour les documents en fran√ßais et un autre pour ceux en anglais. Le format de sortie s√©lectionn√© est le Markdown (`ResultType.MD`), ce qui permet de conserver la structure logique du document original (titres, paragraphes, listes, etc.).

**√âtape 3 : Extraction du Contenu PDF**

On proc√®de ensuite √† l‚Äôextraction effective du contenu des fichiers PDF. LlamaParse utilisant des appels asynchrones, l‚Äôenvironnement doit √™tre adapt√© pour g√©rer cela correctement.

.. code-block:: python
   :linenos:

   import nest_asyncio
   nest_asyncio.apply()

   pdf_files = [("philosophie.pdf", parser_fr)]
   
   with open("plagia_data.md", 'w', encoding='utf-8') as f:
       for file_name, parser in pdf_files:
           print(f"Traitement de {file_name}...")
           documents = parser.load_data(file_name)
           f.write(f"# Contenu extrait de : {file_name}\\n\\n")
           for doc in documents:
               f.write(doc.text + "\\n\\n")

Chaque fichier est trait√© ind√©pendamment. Le texte extrait est structur√© et stock√© dans un fichier Markdown interm√©diaire (`plagia_data.md`). Cela facilite les traitements ult√©rieurs, notamment pour la segmentation en paragraphes ou sections.

**√âtape 4 : Pr√©paration des Donn√©es**

Une fois le contenu extrait, il est lu depuis le fichier Markdown et segment√© en paragraphes. Ces derniers seront convertis en objets `Document`, reconnus par LangChain.

.. code-block:: python
   :linenos:

   with open("plagia_data.md", encoding='utf-8') as f:
       markdown_content = f.read()
   
   paragraphs = [p.strip() for p in markdown_content.split('\\n\\n') if p.strip()]
   documents = [Document(page_content=paragraph) for paragraph in paragraphs]

Chaque double saut de ligne est interpr√©t√© comme une s√©paration logique entre les id√©es ou blocs de contenu. Cette segmentation est cruciale pour que les embeddings soient coh√©rents et repr√©sentatifs du contenu.

**√âtape 5 : G√©n√©ration des Embeddings**

Cette √©tape est centrale : elle convertit le texte en vecteurs num√©riques √† l‚Äôaide d‚Äôun mod√®le d‚Äôembedding compatible avec LangChain. Ces vecteurs sont ensuite stock√©s dans une base Chroma persistante.

.. code-block:: python
   :linenos:

   embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
   
   vecdb = Chroma.from_documents(
       documents=documents,
       embedding=embeddings,
       persist_directory="philo_db",
       collection_name="rag-chroma"
   )
   vecdb.persist()

Le mod√®le utilis√© ici, `mxbai-embed-large:latest`, encode chaque paragraphe en un vecteur dense de 1024 dimensions. Ces vecteurs sont ensuite index√©s et sauvegard√©s localement dans un dossier nomm√© `philo_db`. La collection `rag-chroma` permet de regrouper les documents selon un m√™me th√®me ou usage.

**R√©sultats**

√Ä l'issue de ce processus, une base vectorielle est constitu√©e √† partir du contenu textuel extrait.

.. code-block:: text

   Op√©ration termin√©e avec succ√®s:
   - 914 paragraphes trait√©s
   - Base vectorielle sauvegard√©e dans: philo_db

Cette base peut d√©sormais √™tre utilis√©e pour la recherche s√©mantique, la d√©tection de plagiat ou l‚Äôimpl√©mentation d‚Äôun syst√®me RAG (Retrieval-Augmented Generation).

**Notes Techniques**

- *Format des embeddings* : chaque paragraphe est transform√© en un vecteur de 1024 dimensions, ce qui garantit une bonne expressivit√© s√©mantique.
- *Taille moyenne des paragraphes* : entre 150 et 300 mots, ce qui est optimal pour les mod√®les d‚Äôembedding modernes.
- *M√©tadonn√©es* : il est possible d‚Äôajouter des m√©tadonn√©es √† chaque `Document` (par exemple la langue, l‚Äôorigine du fichier, la section du document, etc.) pour des filtres ou recherches avanc√©es.

**Conclusion**

Ce guide constitue une base robuste pour cr√©er une base vectorielle √† partir de documents PDF multilingues. Il est facilement extensible pour inclure plus de fichiers, enrichir les m√©tadonn√©es ou int√©grer des syst√®mes de recherche s√©mantique avanc√©e.



Application des Approches de Recherche Hybride
==============================================

.. contents:: 
   :depth: 3
   :local:

**Introduction**

La recherche hybride combine plusieurs techniques de similarit√© textuelle pour d√©tecter le plagiat √† diff√©rents niveaux :

1. **Recherche exacte** : D√©tection de copies mot-√†-mot
2. **Similarit√© s√©mantique** : Identification des paraphrases
3. **Analyse multilingue** : Comparaison entre langues (FR‚ÜîEN)

**Architecture Principale**


.. image:: image/3.png
   :alt: architecture de la recherche hybride
   :width: 400px


**Fonctions Cl√©s**

**check_exact_match()**

.. code-block:: python
   :linenos:
   :emphasize-lines: 3-5,12-15

   def check_exact_match(input_text: str, dataset: List[str]) -> List[Tuple[str, float]]:
    """V√©rifie les correspondances exactes avec normalisation avanc√©e"""
    def normalize(text):
        text = re.sub(r'[^\w\s]', '', text.strip().lower())
        return re.sub(r'\s+', ' ', text)
    
    normalized_input = normalize(input_text)
    input_hash = hashlib.md5(normalized_input.encode('utf-8')).hexdigest()
    matches = []
    
    for doc in dataset:
        normalized_doc = normalize(doc)
        doc_hash = hashlib.md5(normalized_doc.encode('utf-8')).hexdigest()
        
        if input_hash == doc_hash:
            return [(doc, 1.0)]
        
        # Similarit√© textuelle ind√©pendante de la langue
        match_ratio = SequenceMatcher(None, normalized_input, normalized_doc).ratio()
        if match_ratio > 0.7:
            matches.append((doc, match_ratio))
        
        # V√©rification des segments longs
        input_words = normalized_input.split()
        doc_words = normalized_doc.split()
        
        for i in range(len(input_words) - 8 + 1):  # Fen√™tre de 8 mots
            segment = ' '.join(input_words[i:i+8])
            if segment in normalized_doc:
                matches.append((doc, max(match_ratio, 0.85)))
                break
    
    unique_matches = {match[0]: match[1] for match in matches}
    return sorted(unique_matches.items(), key=lambda x: x[1], reverse=True) 
   

*Explication* : 

La fonction `check_exact_match` a pour objectif de d√©tecter des correspondances exactes ou partielles entre un texte d'entr√©e (`input_text`) et les documents d'un ensemble de donn√©es (`dataset`), en utilisant une combinaison de techniques de normalisation avanc√©e, de hachage et de similarit√© textuelle.  
Tout d'abord, elle normalise les textes en supprimant les caract√®res sp√©ciaux, en convertissant le texte en minuscules et en r√©duisant les espaces multiples. Ensuite, elle calcule une empreinte MD5 du texte normalis√© pour identifier rapidement des correspondances exactes (score de 1.0). Si aucune correspondance exacte n'est trouv√©e, la fonction √©value la similarit√© textuelle √† l'aide de l'algorithme `SequenceMatcher`, qui compare les s√©quences de caract√®res et retourne un ratio de similarit√© (un score sup√©rieur √† 0.7 est consid√©r√© comme une correspondance partielle).  
Par ailleurs, la fonction v√©rifie la pr√©sence de segments longs (fen√™tres de 8 mots cons√©cutifs) dans le texte d'entr√©e qui pourraient correspondre √† des portions des documents du dataset, attribuant un score minimal de 0.85 dans ce cas. Enfin, les r√©sultats sont d√©dupliqu√©s et tri√©s par score d√©croissant pour fournir une liste ordonn√©e des meilleures correspondances.  


**translate_text()**

.. code-block:: python
   :linenos:

   @st.cache_data(ttl=3600, show_spinner=False)
def translate_text(text: str, target_lang: str) -> str:
    """Traduction intelligente avec gestion des erreurs"""
    try:
        if len(text) < 50:  # Ne pas traduire les textes trop courts
            return text
            
        response = ollama.chat(
            model="llama3.1",
            messages=[{
                "role": "system",
                "content": f"Traduis ce texte en {target_lang} en conservant le sens original:\n{text}"
            }],
            options={'temperature': 0.1}
        )
        return response["message"]["content"]
    except Exception as e:
        st.warning(f"Traduction partielle: {str(e)}")
        return text

*R√¥le* :  

La fonction `translate_text` permet d'effectuer une traduction intelligente d'un texte vers une langue cible (`target_lang`), tout en g√©rant les erreurs potentielles pour assurer une ex√©cution robuste. Elle utilise un mod√®le de traduction bas√© sur `ollama.chat` (avec le mod√®le "llama3.1") pour produire des traductions pr√©cises et contextuelles. Avant toute traduction, la fonction v√©rifie si le texte est trop court (moins de 50 caract√®res) et le retourne tel quel pour √©viter des traductions inutiles ou de mauvaise qualit√©. Si le texte est suffisamment long, elle envoie une requ√™te au mod√®le en sp√©cifiant la langue cible et en demandant une traduction fid√®le au sens original, avec un param√®tre de faible temp√©rature (`temperature: 0.1`) pour favoriser des r√©sultats coh√©rents. En cas d'erreur (comme une panne du service ou un probl√®me de traitement), la fonction affiche un avertissement via `st.warning` et retourne le texte original sans modification, garantissant ainsi qu‚Äôaucune donn√©e ne soit perdue.

**calculate_similarity()**

.. code-block:: python
   :linenos:

   def calculate_similarity(text1: str, text2: str) -> float:
    """Calcule la similarit√© combin√©e TF-IDF + Cross-Encoder"""
    global tfidf_vectorizer
    
    try:
        # Similarit√© lexicale (TF-IDF)
        vectors = tfidf_vectorizer.transform([text1, text2])
        tfidf_sim = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
        
        # Similarit√© s√©mantique (Cross-Encoder)
        cross_score = cross_encoder.predict([[text1, text2]])[0]
        
        # Combinaison pond√©r√©e
        return (cross_score * 0.7) + (tfidf_sim * 0.3)
    except Exception as e:
        st.warning(f"Calcul de similarit√© simplifi√©: {str(e)}")
        return SequenceMatcher(None, text1, text2).ratio()

*Fonctionnement* :  

Coeur de l'approche hybride :

La fonction `calculate_similarity` calcule un score de similarit√© entre deux textes en combinant deux approches compl√©mentaires : une **similarit√© lexicale** (bas√©e sur TF-IDF) et une **similarit√© s√©mantique** (bas√©e sur un mod√®le Cross-Encoder).  

D'abord, la m√©thode **TF-IDF** vectorise les deux textes et mesure leur similarit√© cosinus, ce qui permet d'√©valuer leur ressemblance au niveau des mots et des fr√©quences. Ensuite, un **Cross-Encoder** (mod√®le de deep learning) analyse la signification profonde des textes pour d√©terminer leur proximit√© s√©mantique. Les deux scores sont combin√©s de mani√®re pond√©r√©e (70% pour le Cross-Encoder et 30% pour TF-IDF) afin d'obtenir une mesure √† la fois pr√©cise et nuanc√©e.  

En cas d'erreur (par exemple, si le vectoriseur TF-IDF ou le mod√®le Cross-Encoder n'est pas disponible), la fonction utilise une m√©thode de repli plus simple (`SequenceMatcher`), qui compare les s√©quences de caract√®res pour fournir un ratio de similarit√© basique, tout en affichant un avertissement pour informer l'utilisateur. 

**hybrid_search()**

.. code-block:: python
   :linenos:
   :emphasize-lines: 8-9,15-17,25-27

  def hybrid_search(query: str, dataset: List[str], top_k: int = 10) -> List[Dict[str, Any]]:
    """Recherche hybride multilingue avec gestion des erreurs"""
    global vecdb
    
    try:
        # D√©tection de la langue de la requ√™te
        query_lang = detect(query) if len(query) > 20 else 'en'
        
        # 1. V√©rifier les copies exactes
        exact_matches = check_exact_match(query, dataset)
        if exact_matches:
            return [{
                "content": match[0],
                "similarity": match[1],
                "match_type": "exact",
                "metadata": {},
                "combined_score": match[1]
            } for match in exact_matches[:top_k]]
        
        # 2. Recherche dans la langue d'origine
        vector_results = vecdb.similarity_search_with_score(query, k=top_k*2)
        
        # 3. Si la requ√™te est en fran√ßais, chercher aussi en anglais et vice versa
        translated_results = []
        if query_lang == 'fr':
            translated_query = translate_text(query, 'en')
            if translated_query != query:
                translated_results = vecdb.similarity_search_with_score(translated_query, k=top_k)
        elif query_lang == 'en':
            translated_query = translate_text(query, 'fr')
            if translated_query != query:
                translated_results = vecdb.similarity_search_with_score(translated_query, k=top_k)
        
        # Combiner les r√©sultats
        all_results = []
        
        # Ajouter les r√©sultats originaux
        for doc, score in vector_results:
            sim_score = calculate_similarity(query, doc.page_content)
            all_results.append({
                "content": doc.page_content,
                "similarity": sim_score,
                "match_type": "semantic",
                "metadata": doc.metadata,
                "combined_score": sim_score
            })
        
        # Ajouter les r√©sultats traduits
        for doc, score in translated_results:
            translated_content = translate_text(doc.page_content, query_lang)
            sim_score = calculate_similarity(query, translated_content)
            all_results.append({
                "content": doc.page_content,
                "similarity": sim_score,
                "match_type": "translated",
                "metadata": doc.metadata,
                "combined_score": sim_score * 0.9  # L√©g√®re p√©nalit√© pour la traduction
            })
        
        # √âliminer les doublons et trier
        unique_results = {}
        for res in all_results:
            if res["content"] not in unique_results or res["combined_score"] > unique_results[res["content"]]["combined_score"]:
                unique_results[res["content"]] = res
        
        return sorted(unique_results.values(), key=lambda x: x["combined_score"], reverse=True)[:top_k]
    
    except Exception as e:
        st.error(f"Erreur de recherche: {str(e)}")
        return []

*fonctionnement* :

La fonction **`hybrid_search`** impl√©mente un syst√®me de recherche hybride multilingue qui combine plusieurs techniques pour retrouver les documents les plus pertinents dans un jeu de donn√©es en fonction d'une requ√™te utilisateur.  
D'abord, elle d√©tecte automatiquement la langue de la requ√™te (sauf si le texte est trop court, auquel cas elle suppose l'anglais par d√©faut). Ensuite, elle v√©rifie s'il existe des **correspondances exactes** dans le dataset en utilisant la fonction `check_exact_match`, ce qui permet d'identifier rapidement des r√©pliques identiques ou quasi-identiques avec un score de confiance maximal (1.0).  
Si aucune correspondance exacte n'est trouv√©e, la fonction effectue une **recherche s√©mantique** en utilisant un syst√®me de plongements vectoriels (`vecdb.similarity_search_with_score`) pour trouver des documents similaires dans la langue d'origine. Pour am√©liorer les r√©sultats, elle propose √©galement une **recherche multilingue** : si la requ√™te est en fran√ßais, elle la traduit en anglais (et inversement) puis relance une recherche s√©mantique sur cette version traduite.  
Les r√©sultats sont ensuite combin√©s, √©valu√©s avec un **score de similarit√© hybride** (int√©grant √† la fois la similarit√© lexicale et s√©mantique via `calculate_similarity`), puis d√©dupliqu√©s pour √©viter les doublons. Les documents traduits subissent une l√©g√®re p√©nalit√© (coefficient 0.9) pour privil√©gier les r√©sultats dans la langue originale. Enfin, les r√©sultats sont tri√©s par pertinence d√©croissante et renvoy√©s sous forme d'une liste de dictionnaires contenant le contenu, le score, le type de correspondance et des m√©tadonn√©es associ√©es.  
En cas d'erreur, la fonction affiche un message d'alerte via `st.error` et retourne une liste vide pour √©viter toute interruption brutale du processus.


**analyze_ideas()**

.. code-block:: python
   :linenos:

   def analyze_ideas(input_text: str, matches: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Analyse des similarit√©s conceptuelles entre phrases"""
    ideas = []
    sentences = [s.strip() for s in re.split(r'[.!?]', input_text) if len(s.strip().split()) > 5]
    
    for match in matches:
        if match["combined_score"] < 0.4:  # Seuil pour les id√©es similaires
            continue
            
        match_sentences = [s.strip() for s in re.split(r'[.!?]', match["content"]) if len(s.strip().split()) > 5]
        
        for sent in sentences:
            for match_sent in match_sentences:
                sim_score = calculate_similarity(sent, match_sent)
                if sim_score > 0.5:  # Seuil pour similarit√© d'id√©e
                    ideas.append({
                        "source_sentence": sent,
                        "matched_sentence": match_sent,
                        "similarity": sim_score,
                        "source_content": match["content"][:200] + "...",
                        "metadata": match.get("metadata", {})
                    })
    
    # Regrouper les id√©es similaires
    grouped_ideas = defaultdict(list)
    for idea in ideas:
        key = idea["source_sentence"][:50]  # Regrouper par phrase source
        grouped_ideas[key].append(idea)
    
    # Garder la meilleure correspondance pour chaque groupe
    return [max(group, key=lambda x: x["similarity"]) for group in grouped_ideas.values()]

*R√¥le*: 

La fonction **`analyze_ideas`** permet d'identifier et d'analyser les similarit√©s conceptuelles entre un texte d'entr√©e et une liste de documents pr√©-appari√©s. Elle commence par d√©couper le texte source et les documents en phrases pertinentes (en excluant les segments trop courts), puis √©value leurs relations s√©mantiques √† l'aide d'un score de similarit√© combinant approche lexicale et s√©mantique. Seules les correspondances significatives (d√©passant un seuil de 0.5) sont conserv√©es, √©vitant ainsi les faux positifs. Les r√©sultats sont ensuite organis√©s par groupe d'id√©es similaires, en ne gardant que la meilleure correspondance pour chaque phrase source. La sortie inclut non seulement les paires de phrases similaires et leur score, mais aussi un extrait du document d'origine et ses m√©tadonn√©es, offrant ainsi un contexte clair pour chaque rapprochement identifi√©.

**Visualisation des R√©sultats**

**create_similarity_network()**

.. code-block:: python
   :linenos:

   def create_similarity_network(matches: List[Dict[str, Any]]) -> str:
    """Cr√©e un r√©seau de similarit√© interactif"""
    G = nx.Graph()
    
    for i, match in enumerate(matches):
        G.add_node(f"Source_{i}", size=15, color='blue')
        G.add_node(match['metadata'].get('source', f"Doc_{i}"), size=10, color='red')
        G.add_edge(f"Source_{i}", match['metadata'].get('source', f"Doc_{i}"), 
                  weight=match['combined_score'], title=f"{match['combined_score']:.2f}")
    
    net = Network(height="500px", width="100%", bgcolor="#222222", font_color="white")
    net.from_nx(G)
    
    # Sauvegarde temporaire pour affichage
    path = tempfile.mkdtemp()
    net.save_graph(f"{path}/network.html")
    
    return open(f"{path}/network.html").read()

*R√¥le* :  

La fonction **`create_similarity_network`** transforme des r√©sultats d'analyse textuelle en une visualisation interactive sous forme de r√©seau, permettant d'explorer intuitivement les relations entre diff√©rents documents. Elle construit un graphe o√π chaque phrase source appara√Æt comme un n≈ìud bleu, tandis que les documents appari√©s sont repr√©sent√©s par des n≈ìuds rouges. Les connexions entre ces √©l√©ments, mat√©rialis√©es par des ar√™tes dont l'√©paisseur varie selon l'intensit√© de la similarit√©, r√©v√®lent la structure des relations s√©mantiques au sein du corpus. 
Gr√¢ce √† l'int√©gration de la biblioth√®que `pyvis`, le r√©seau offre une interactivit√© riche : l'utilisateur peut survoler les liens pour voir les scores pr√©cis, r√©organiser dynamiquement la disposition des n≈ìuds, ou zoomer sur des zones d'int√©r√™t, le tout pr√©sent√© sur un fond sombre optimis√© pour la lisibilit√©. Le graphe, g√©n√©r√© au format HTML dans un r√©pertoire temporaire, peut √™tre facilement incorpor√© √† des tableaux de bord ou applications web. 
Cette approche visuelle est particuli√®rement utile pour identifier rapidement des clusters th√©matiques, rep√©rer des documents centraux dans un r√©seau d'id√©es, ou explorer les relations entre diff√©rents textes. Elle sert ainsi de pont entre une analyse quantitative rigoureuse (bas√©e sur les scores de similarit√©) et une interpr√©tation qualitative facilit√©e par la repr√©sentation spatiale des donn√©es textuelles.

**Conclusion**

Cette approche hybride combine :

- *Pr√©cision* : D√©tection des copies exactes
- *Nuance* : Compr√©hension s√©mantique
- *Couverure* : Analyse multilingue
- *Transparence* : Visualisations explicatives

Cr√©ation d'une interface streamlit 
==================================

Cette partie d√©taille la conception et l'impl√©mentation d'une interface Streamlit compl√®te pour une application de d√©tection de plagiat AI-powered.

**Introduction**

L'interface Streamlit a √©t√© con√ßue pour offrir une exp√©rience utilisateur riche avec :

- Un dashboard interactif
- Des visualisations de donn√©es avanc√©es
- Une analyse en temps r√©el
- Un design responsive et moderne


**Configuration Initiale**

.. code-block:: python

    import streamlit as st
    st.set_page_config(
        layout="wide", 
        page_title="üîç AI Plagiarism Sentinel Pro", 
        page_icon="üîç"
    )

*Explications :*

- ``layout="wide"`` permet d'utiliser toute la largeur de l'√©cran
- Personnalisation du titre et de l'ic√¥ne pour une identit√© visuelle

**Initialisation des Mod√®les**

.. code-block:: python

    @st.cache_resource(show_spinner=False)
    def initialize_system():
        # Initialisation des embeddings
        embeddings = OllamaEmbeddings(
            model="mxbai-embed-large:latest",
            temperature=0.01,
            top_k=50
        )
        
        # Initialisation de la base vectorielle
        vecdb = Chroma(
            persist_directory="philo_db",
            embedding_function=embeddings,
            collection_name="rag-chroma"
        )

*Explications :*

- ``@st.cache_resource`` optimise les performances en cachant les ressources initialis√©es
- La fonction charge les mod√®les NLP et la base de donn√©es vectorielle

**Interface Utilisateur**

  **- En-t√™te Personnalis√©**

.. code-block:: python

    def load_assets():
        try:
            response = requests.get("https://images.unsplash.com/photo-1620712943543-bcc4688e7485")
            banner = Image.open(BytesIO(response.content))
            return banner
        except:
            return None

    banner_image = load_assets()
    if banner_image:
        st.image(banner_image, use_column_width=True)
    else:
        st.markdown("""
        <div class="header">
            <h1>üîç AI Plagiarism Sentinel Pro</h1>
        </div>
        """, unsafe_allow_html=True)

*Explications :*

- T√©l√©chargement dynamique d'une banni√®re
- Fallback sur un en-t√™te HTML si l'image n'est pas disponible

  **-Sidebar Configurable**

.. code-block:: python

    with st.sidebar:
        st.title("‚öôÔ∏è Param√®tres Experts")
        
        with st.expander("üîç Options de Recherche", expanded=True):
            analysis_mode = st.selectbox(
                "Mode d'analyse",
                ["DeepScan Pro", "Rapide", "Manuel Expert"]
            )
            
            sensitivity = st.slider(
                "Niveau de sensibilit√©",
                1, 10, 8
            )

*Explications :*

- Organisation des contr√¥les dans des expanders
- Utilisation de widgets Streamlit vari√©s (selectbox, slider)

 **- Zone de Saisie Multimode**

.. code-block:: python

    input_method = st.radio(
        "Source d'entr√©e",
        ["üìù Texte direct", "üìÇ Fichier", "üåê URL"],
        horizontal=True
    )
    
    if input_method == "üìÇ Fichier":
        uploaded_file = st.file_uploader(
            "T√©l√©versez un document",
            type=["txt", "pdf", "docx"]
        )

*Explications :*

- Interface unifi√©e pour diff√©rentes m√©thodes de saisie
- Traitement sp√©cifique pour chaque type d'entr√©e
**Visualisations Avanc√©es**

  **- Cartes de R√©sultats**

.. code-block:: python

    def display_match_card(match):
        with st.container():
            st.markdown(f"""
            <div class="{'exact-match' if match['match_type'] == 'exact' else 'partial-match'}">
                <h3>{match['match_type'].capitalize()} - Score: {match['combined_score']*100:.1f}%</h3>
                <p><strong>Source:</strong> {match['metadata'].get('source', 'Inconnue')}</p>
            </div>
            """, unsafe_allow_html=True)

*Explications :*

- Utilisation de HTML/CSS pour des cartes stylis√©es
- Classes CSS dynamiques en fonction du type de correspondance

  **-R√©seau de Similarit√©**

.. code-block:: python

    def create_similarity_network(matches):
        G = nx.Graph()
        for i, match in enumerate(matches):
            G.add_node(f"Source_{i}", size=15, color='blue')
            G.add_edge(f"Source_{i}", match['metadata'].get('source', f"Doc_{i}"), 
                      weight=match['combined_score'])
        
        net = Network(height="500px", width="100%")
        net.from_nx(G)
        return net

*Explications :*

- Utilisation de NetworkX pour la cr√©ation du graphe
- Int√©gration avec PyVis pour le rendu interactif

**Gestion des Donn√©es**

  **- Cache et Performance**

.. code-block:: python

    @st.cache_data(ttl=3600)
    def translate_text(text: str, target_lang: str) -> str:
        # Fonction de traduction
        return translated_text

*Explications :*

- ``@st.cache_data`` pour cacher les r√©sultats co√ªteux
- TTL (Time-To-Live) de 1 heure pour les traductions

  **- Traitement des Fichiers**

.. code-block:: python

    if uploaded_file.type == "application/pdf":
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = "\n".join([page.extract_text() for page in pdf_reader.pages])
    elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        text = docx2txt.process(uploaded_file)

*Explications :*

- Support multi-format (PDF, DOCX, etc.)
- Extraction robuste du texte

**Design Avanc√©**

  **- CSS Personnalis√©**

.. code-block:: python

    def apply_custom_css():
        css = """
        <style>
            .header {
                background: linear-gradient(135deg, #434343 0%, #000000 100%);
                color: white;
                padding: 2rem;
                border-radius: 10px;
            }
            .exact-match { 
                border-left: 6px solid #ef4444; 
                background-color: rgba(239, 68, 68, 0.05);
            }
        </style>
        """
        st.markdown(css, unsafe_allow_html=True)

*Explications :*

- Styles CSS int√©gr√©s directement dans Streamlit
- Utilisation de gradients et d'effets modernes

**Mise en Page**

.. code-block:: python

    col1, col2 = st.columns(2)
    with col1:
        st.metric("Score maximal", f"{score:.1f}%")
    with col2:
        st.metric("Correspondances", count)

*Explications :*

- Layout multi-colonnes pour une organisation optimale
- Widgets de m√©triques pour les KPI

**Fonctionnalit√©s Avanc√©es**

  **- Onglets Interactifs**

.. code-block:: python

    tab1, tab2 = st.tabs(["üìä Dashboard", "üîç Correspondances"])
    with tab1:
        st.plotly_chart(fig)
    with tab2:
        for match in matches:
            display_match_card(match)

*Explications :*

- Navigation par onglets pour organiser le contenu
- Contenu dynamique dans chaque onglet

  **- G√©n√©ration de Rapports**

.. code-block:: python

    def generate_full_report(results):
        return json.dumps({
            "metadata": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_matches": len(results.get('all_matches', []))
            },
            "results": {
                "highest_score": results.get('highest_score', 0),
            }
        }, indent=2)

*Explications :*

- Format JSON structur√©
- T√©l√©chargement direct via Streamlit

**Conclusion**

Cette interface Streamlit combine :

- Des composants UI riches
- Des visualisations interactives
- Une gestion efficace des donn√©es
- Un design moderne personnalisable

Les techniques pr√©sent√©es peuvent √™tre adapt√©es pour tout type d'application data-centric.

R√©sultats
=========

pour connaitre la performance des mod√®les  , nous avons test√© notre application par diff√©rents formes , un texte d√©ja d√©ja en pdf mais en une autre language , un texte similaire , un texte similaire mais par des mots diff√©rents pour tester le cot√© s√©mantique , un texte qui a une id√©√© simialaire √† une id√©e d√©ja en pdf , un text qui est tr√©s loin de dataset pour montrer de non-plagiat , nous avons aussi test√© application par des textes sous forme "txt", "pdf", "docx", et elle donne des bonnes r√©sultats

voici les r√©sultats sur streamlit  d' un exemple :(entrer un texte similaire √† un texte de dataset avec la changement de quelques mots)

.. image:: image/P1.png
   :alt: a
   :width: 900px

**voici l'interface initial de notre application en streamlit**



.. image:: image/P2.png
   :alt: b
   :width: 900px

**nous avons entr√© un texte similaire avec changement de quelques mots dans text direct**




.. image:: image/P3.png
   :alt: c
   :width: 900px

**voici le resultat g√©n√©ral qui nous d√©clare que la plagiat est √©vident d'un score de 85% et un dashboard qui donne les pourcentage de similitude et d'original**



.. image:: image/P4.png
   :alt: d
   :width: 900px

**voici les principales correspondances  avec chaque texte qui est en dataset qui est correspondant a chaque extrait analys√© de texte d'entr√© avec un score de plagiat**



.. image:: image/P5.png
   :alt: e
   :width: 900px

**cette visualisation pour les id√©√©s qui sont conceptuellement similaires,et elle affiche chaque id√©e qui est en dataset qui est correspondant a chaque id√©e de texte d'entr√© avec un score de similarit√©**




.. image:: image/P6.png
   :alt: f
   :width: 900px

**voici une liste compl√®te bien r√©dig√© de correspondances avec ses options d'affichages, score minimum √† afficher et type de correspondance (exact,semantic,transleted)**



.. image:: image/P9.png
   :alt: g
   :width: 900px

**dans visualisations , on trouve le r√©seau de similarit√© qui relie chaque docs de l'entr√©e √† une source (database vectorielle)**



.. image:: image/P7.png
   :alt: h
   :width: 900px

**ce diagramme √† barre montre que notre texte d'entr√©e est de type copie exacte**



.. image:: image/P8.png
   :alt: i
   :width: 900px

**dans rapport complet , tu peux voir le rapport complet de r√©sultat ou tu peux aussi le t√©lechager , avec un r√©sum√© ex√©cutif ,et enfin une recommandation (conseil)**


Travaux futurs
==============

Cette partie pr√©sente les am√©liorations potentielles pour la future version du syst√®me de d√©tection de plagiat.

**1. Am√©liorations des Algorithmes**

*1.1. Int√©gration de Mod√®les Multilingues Avanc√©s*

- Ajout de mod√®les sp√©cialis√©s pour d'autres langues (espagnol, allemand, chinois)
- Impl√©mentation d'un syst√®me de d√©tection automatique de langue plus robuste
- Optimisation des traductions avec des mod√®les d√©di√©s (NLLB, DeepL)

*1.2. Am√©lioration des Scores de Similarit√©*
- Combinaison de plusieurs m√©triques (BERTScore, ROUGE, BLEU)
- Ajout d'un syst√®me de pond√©ration dynamique bas√© sur le contexte
- Int√©gration de mod√®les de similarit√© sp√©cifiques aux domaines (scientifique, juridique)

**2. Fonctionnalit√©s Avanc√©es**

*2.1. Analyse Temporelle*

- D√©tection des variations stylistiques dans le texte
- Identification des ajouts/modifications successifs
- Reconstruction de l'historique d'√©criture

*2.2. D√©tection de Paraphrase Sophistiqu√©e*

- Mod√®les sp√©cifiques pour identifier les paraphrases avanc√©es
- D√©tection des modifications structurelles (changement d'ordre des id√©es)
- Analyse des patterns de r√©√©criture

**3. Interface Utilisateur**

*3.1. Tableau de Bord Analytique*

- Visualisations interactives des r√©sultats
- Comparaison avec les soumissions pr√©c√©dentes
- Suivi des am√©liorations dans les r√©visions

*3.2. Outils d'Aide √† la R√©√©criture*

- Suggestions de reformulation originales
- G√©n√©rateur de citations automatiques
- Identification des passages √† risque

**4. Infrastructure Technique**

*4.1. Optimisation des Performances*

- Impl√©mentation d'un syst√®me de cache distribu√©
- Pr√©traitement asynchrone des documents
- Indexation incr√©mentielle

*4.2. Extension des Bases de R√©f√©rence*

- Int√©gration de nouvelles sources acad√©miques
- Connexion aux bases de donn√©es ouvertes
- Mise √† jour automatique du corpus de r√©f√©rence

**5. Int√©grations Syst√®me**

*5.1. API Universelle*

- D√©veloppement d'une API RESTful compl√®te
- Int√©gration avec les LMS (Moodle, Canvas)
- Connecteurs pour les outils d'√©dition (Word, Google Docs)

*5.2. Modules Sp√©cialis√©s*

- Version pour l'√©dition scientifique
- Module d√©di√© √† l'√©ducation
- Solution pour les √©diteurs professionnels

**Perspectives √† Long Terme**

- Analyse multimodale (texte + images + formules)
- D√©tection cross-m√©dia (vid√©os, podcasts)
- Syst√®me pr√©dictif de risque de plagiat
- Blockchain pour la tra√ßabilit√© des sources

Ces am√©liorations permettront de positionner l'outil comme une solution compl√®te de v√©rification d'int√©grit√© acad√©mique et professionnelle.


Conclusion
==========

Apr√®s la r√©alisation de ce projet AI Plagiarism Sentinel Pro, plusieurs constats importants peuvent √™tre tir√©s :

1. **Efficacit√© de d√©tection** : 
   Le syst√®me combine avec succ√®s diff√©rentes approches (correspondance exacte, analyse s√©mantique, similarit√© conceptuelle) pour offrir une d√©tection de plagiat multi-niveaux tr√®s performante.

2. **Innovation technologique** :
   L'utilisation combin√©e de mod√®les de langue (Ollama), d'embeddings vectoriels et de techniques traditionnelles (TF-IDF) permet une analyse √† la fois profonde et rapide.

3. **Polyvalence linguistique** :
   La capacit√© √† traiter plusieurs langues (notamment fran√ßais et anglais) et √† identifier des similarit√©s translinguistiques constitue un atout majeur.

4. **Analyse stylistique** :
   Les fonctionnalit√©s d'analyse d'√©criture vont au-del√† de la simple d√©tection de plagiat, offrant des insights pr√©cieux sur le style et la qualit√© r√©dactionnelle.

5. **Interface intuitive** :
   Le dashboard Streamlit propose une exp√©rience utilisateur riche tout en restant accessible, avec des visualisations claires et des rapports d√©taill√©s.


