================================================
Documentation du projet IA de plagiarism checker
================================================

Bienvenue dans la documentation du projet D√©tection de plagiat par Intelligence Artificielle √† l'aide de RAG et d'analyse s√©mantique avanc√©e.Ce document d√©taille les m√©thodologies, les outils utilis√©s, ainsi que les r√©sultats obtenus pour identifier les cas de plagiat, qu‚Äôils soient exacts, paraphras√©s ou s√©mantiquement similaires.L‚Äôobjectif de ce projet est de d√©velopper un syst√®me de d√©tection robuste en combinant plusieurs approches

*Table des mati√®res*

  - introduction
  - objectifs du projet
  - installation
  - pipeline 
  - creation d'une base de donn√©s vectorielle a partir de llama_parse 
  - application des approches (recherche hybride)
  - visualisation des r√©sultat 
  - cr√©ation d'une interface streamlit 
  - Travaux Futurs
  - conclusion


.. AI Plagiarism Sentinel Pro documentation master file

==============================================
AI Plagiarism Sentinel Pro ‚Äì D√©tection avanc√©e
==============================================

Introduction
============

La d√©tection automatique du plagiat est devenue un enjeu crucial dans le monde acad√©mique et professionnel, o√π la v√©rification de l'originalit√© d'un contenu est essentielle √† la cr√©dibilit√© intellectuelle et √† l‚Äôint√©grit√© des travaux. 

Ce projet propose une solution intelligente et robuste de d√©tection de plagiat en temps r√©el, exploitant les derni√®res avanc√©es en **traitement du langage naturel (NLP)**, **recherche vectorielle**, et **mod√®les de similarit√© s√©mantique multilingue**.

En s‚Äôappuyant sur une combinaison d‚Äô**embeddings puissants**, de **moteurs de recherche hybride**, de **mod√®les cross-encoders**, ainsi que d‚Äôune **analyse stylistique avanc√©e**, cette plateforme permet de d√©tecter diff√©rents types de plagiat : copie exacte, paraphrase, similitude conceptuelle, ou correspondance multilingue.

Le tout est encapsul√© dans une interface interactive d√©velopp√©e avec **Streamlit**, offrant √† l'utilisateur une exp√©rience fluide, visuelle, et totalement explicable.

Objectifs du projet
===================

- **Fournir un syst√®me de d√©tection de plagiat automatis√© et intelligent** :
  
  - Bas√© sur une base vectorielle *Chroma* enrichie par des embeddings (*Ollama embeddings*).
  - Capable d‚Äôidentifier non seulement des copies exactes, mais aussi des paraphrases et similitudes conceptuelles, y compris entre langues diff√©rentes (fran√ßais/anglais).

- **D√©montrer l‚Äôutilisation combin√©e de technologies modernes** :
  
  - üß† *LangChain*, *TF-IDF*, *Cross-Encoder* (MS-MARCO) pour l‚Äôanalyse s√©mantique.
  - üìö *SpaCy* et *textstat* pour l‚Äôanalyse stylistique, lisibilit√©, structure grammaticale, diversit√© lexicale.
  - üìä *Streamlit*, *Plotly*, *WordCloud*, *Pyvis* pour la visualisation avanc√©e et l‚Äôexplicabilit√©.

- **Assurer une analyse compl√®te et interpr√©table** :
  
  - G√©n√©ration d‚Äôun rapport technique JSON d√©taill√©.
  - R√©sum√© ex√©cutif avec score global, alertes de plagiat, et recommandations.
  - Visualisation des correspondances (r√©seaux interactifs, nuages de mots, graphiques).

- **Garantir une exp√©rience utilisateur professionnelle** :
  
  - Interface responsive avec personnalisation CSS.
  - Chargement de texte via saisie directe, fichiers (.pdf, .docx‚Ä¶), ou URL.
  - T√©l√©chargement dynamique de rapports avec score et diagnostic.




Installation
============

Les biblioth√®ques suivantes sont n√©cessaires pour le projet :

1. **os** : Manipulation des fichiers et chemins.
2. **pickle** : S√©rialisation et sauvegarde des objets Python.
3. **cv2** : Traitement d'images avec OpenCV.
4. **numpy** : Manipulation de matrices et calculs num√©riques.
5. **streamlit** : Interface web interactive pour l‚Äôutilisateur.
6. **langchain.vectorstores** : Gestion des bases vectorielles avec Chroma.
7. **langchain_community.embeddings** : G√©n√©ration des embeddings avec Ollama.
8. **langdetect** : D√©tection automatique de la langue d‚Äôun texte.
9. **typing** : Annotations de types (`List`, `Dict`, etc.).
10. **time** : Gestion du temps (dur√©e d‚Äôanalyse, timestamps).
11. **pandas** : Manipulation et affichage de tableaux de donn√©es.
12. **difflib** : Comparaison de cha√Ænes pour la similarit√© exacte.
13. **matplotlib.pyplot** : Visualisation de donn√©es.
14. **plotly.express** : Graphiques interactifs (camemberts, barres).
15. **collections.defaultdict** : Regroupement d‚Äô√©l√©ments similaires.
16. **re** : Expressions r√©guli√®res pour le nettoyage de texte.
17. **hashlib** : Hachage des textes pour la d√©tection exacte.
18. **sentence_transformers** : Calcul avanc√© de similarit√© s√©mantique (CrossEncoder).
19. **sklearn.feature_extraction.text** : TF-IDF vectorisation.
20. **sklearn.metrics.pairwise** : Similarit√© cosinus.
21. **PIL.Image** : Chargement et affichage d‚Äôimages.
22. **requests** : Requ√™te HTTP pour charger des images ou contenus.
23. **io.BytesIO** : Manipulation de contenu binaire.
24. **json** : S√©rialisation JSON pour les rapports.
25. **networkx** : Cr√©ation de graphes de similarit√©.
26. **pyvis.network** : Visualisation interactive de r√©seaux.
27. **tempfile** : Cr√©ation de fichiers temporaires.
28. **spacy** : Analyse grammaticale et entit√©s nomm√©es.
29. **wordcloud.WordCloud** : Nuage de mots bas√© sur les correspondances.
30. **textstat** : Analyse de lisibilit√©.
31. **streamlit.components.v1.html** : Affichage de composants HTML personnalis√©s.
32. **docx2txt** : Extraction de texte depuis fichiers Word.
33. **PyPDF2** : Extraction de texte depuis fichiers PDF.
34. **base64** : Encodage d‚Äôimages pour l‚Äôaffichage CSS.
35. **annotated_text** : Mise en √©vidence de texte dans Streamlit.
36. **st_aggrid** : Tableaux interactifs dans Streamlit.
37. **ollama** : Requ√™tes vers un mod√®le de langage local.

.. code-block:: python

   import os
   import pickle
   import cv2
   import numpy as np
   import streamlit as st
   import time
   import pandas as pd
   import matplotlib.pyplot as plt
   import plotly.express as px
   import re
   import hashlib
   import json
   import tempfile
   import requests
   import base64
   import docx2txt
   import PyPDF2
   import networkx as nx
   from PIL import Image
   from io import BytesIO
   from difflib import SequenceMatcher
   from collections import defaultdict
   from typing import List, Dict, Any, Tuple
   from sentence_transformers import CrossEncoder
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.metrics.pairwise import cosine_similarity
   from langchain.vectorstores import Chroma
   from langchain_community.embeddings import OllamaEmbeddings
   from wordcloud import WordCloud
   import spacy
   import textstat
   from streamlit.components.v1 import html
   from annotated_text import annotated_text
   from st_aggrid import AgGrid
   import ollama


pipeline
========
.. list-table::
   :widths: 100 200
   :align: center

   * - .. image:: image/1.png
         :alt: Image 1
         :width: 700px
     - .. image:: image/2.png
         :alt: Image 2
         :width: 700px

explication de pipline:
-----------------------
Phase 1: Pr√©paration de la Base Vectorielle
-------------------------------------------

.. list-table::
   :header-rows: 1
   :widths: 10 30 60

   * - √âtape
     - Outils/M√©thodes
     - Description
   * - 1. Extraction
     - LlamaParse (FR/EN)
     - Conversion des PDF/DOCX en Markdown propre
   * - 2. Nettoyage
     - Regex + Unicode Normalization
     - Suppression des en-t√™tes, pieds de page, caract√®res sp√©ciaux
   * - 3. D√©coupage
     - ``split('\\n\\n')``
     - S√©paration en paragraphes (1 paragraphe = 1 document)
   * - 4. Embeddings
     - OllamaEmbeddings (mxbai-embed-large)
     - Vectorisation des paragraphes
   * - 5. Stockage
     - Chroma DB
     - Indexation avec m√©tadonn√©es (source, langue)
   * - 6. Persistance
     - ``vecdb.persist()``
     - Sauvegarde locale dans ``philo_db``

Phase 2: Analyse de Plagiat (Frontend/Backend)
----------------------------------------------

.. list-table::
   :header-rows: 1
   :widths: 10 30 60

   * - √âtape
     - Outils/M√©thodes
     - Description
   * - 1. Input Utilisateur
     - Streamlit (``file_uploader``/``text_area``)
     - Support pour texte direct, fichiers, ou URLs
   * - 2. Pr√©-processing
     - ``langdetect`` + ``spacy``
     - D√©tection de langue et nettoyage
   * - 3. Recherche Hybride
     - Combinaison de 3 m√©thodes:
     - 
   * - 
     - ‚Ä¢ **Exact Match** (MD5 + ``SequenceMatcher``)
     - D√©tection de copies mot-√†-mot
   * - 
     - ‚Ä¢ **Semantic Search** (Ollama + Cross-Encoder)
     - Similarit√© conceptuelle (seuil: 0.4-1.0)
   * - 
     - ‚Ä¢ **Multilingue** (Traduction via Llama3)
     - Comparaison FR‚ÜîEN
   * - 4. Post-Traitement
     - ``networkx`` + ``pyvis``
     - G√©n√©ration du r√©seau de similarit√©
   * - 5. Rapport
     - JSON + Streamlit
     - Export des r√©sultats d√©taill√©s


Cr√©ation d'une Base de Donn√©es Vectorielle avec LlamaParse
===========================================================

Ce guide fournit une proc√©dure compl√®te pour transformer un ou plusieurs fichiers PDF en une base de donn√©es vectorielle, utilisable notamment pour la d√©tection de similarit√© textuelle ou de plagiat. L'approche repose sur l'utilisation combin√©e de **LlamaParse** pour l'extraction intelligente de texte structur√© et de **LangChain** pour la vectorisation et la gestion des documents.

.. contents:: Sommaire
   :depth: 2
   :local:

√âtape 1 : Installation des D√©pendances
--------------------------------------

Cette premi√®re √©tape consiste √† importer l'ensemble des librairies n√©cessaires au bon fonctionnement du pipeline. 

.. code-block:: python
   :linenos:

   import os
   from llama_parse import LlamaParse
   from llama_parse.base import ResultType
   from langchain.text_splitter import RecursiveCharacterTextSplitter
   from langchain.vectorstores import Chroma
   from langchain.embeddings import HuggingFaceEmbeddings
   from langchain_core.documents import Document
   from llama_cloud_services.parse.utils import Language
   from langchain_community.embeddings.ollama import OllamaEmbeddings

Les modules import√©s remplissent des r√¥les sp√©cifiques :
- `llama_parse` permet d'extraire le contenu structur√© des PDF (en Markdown ici).
- `langchain` permet de g√©rer la transformation du texte en vecteurs ainsi que leur stockage dans une base.
- `OllamaEmbeddings` fournit un mod√®le d'embedding performant pour convertir du texte en vecteurs num√©riques.

√âtape 2 : Configuration de l'API LlamaParse
-------------------------------------------

Avant de lancer l'extraction, il est n√©cessaire de configurer LlamaParse avec une cl√© API valide. On peut √©galement sp√©cifier la langue du document pour am√©liorer la pr√©cision de l‚Äôanalyse.

.. code-block:: python
   :linenos:

   os.environ["LLAMA_CLOUD_API_KEY"] = "llx-a2C7FgYfP1hzX3pXuvtdaNmexAqsuRnJIJ2G6MjbBrfuS3QY"
   
   parser_fr = LlamaParse(
       result_type=ResultType.MD, 
       language=Language.FRENCH
   )
   parser_en = LlamaParse(
       result_type=ResultType.MD,
       language=Language.ENGLISH
   )

Deux parseurs sont initialis√©s ici : un pour les documents en fran√ßais et un autre pour ceux en anglais. Le format de sortie s√©lectionn√© est le Markdown (`ResultType.MD`), ce qui permet de conserver la structure logique du document original (titres, paragraphes, listes, etc.).

√âtape 3 : Extraction du Contenu PDF
-----------------------------------

On proc√®de ensuite √† l‚Äôextraction effective du contenu des fichiers PDF. LlamaParse utilisant des appels asynchrones, l‚Äôenvironnement doit √™tre adapt√© pour g√©rer cela correctement.

.. code-block:: python
   :linenos:

   import nest_asyncio
   nest_asyncio.apply()

   pdf_files = [("philosophie.pdf", parser_fr)]
   
   with open("plagia_data.md", 'w', encoding='utf-8') as f:
       for file_name, parser in pdf_files:
           print(f"Traitement de {file_name}...")
           documents = parser.load_data(file_name)
           f.write(f"# Contenu extrait de : {file_name}\\n\\n")
           for doc in documents:
               f.write(doc.text + "\\n\\n")

Chaque fichier est trait√© ind√©pendamment. Le texte extrait est structur√© et stock√© dans un fichier Markdown interm√©diaire (`plagia_data.md`). Cela facilite les traitements ult√©rieurs, notamment pour la segmentation en paragraphes ou sections.

√âtape 4 : Pr√©paration des Donn√©es
---------------------------------

Une fois le contenu extrait, il est lu depuis le fichier Markdown et segment√© en paragraphes. Ces derniers seront convertis en objets `Document`, reconnus par LangChain.

.. code-block:: python
   :linenos:

   with open("plagia_data.md", encoding='utf-8') as f:
       markdown_content = f.read()
   
   paragraphs = [p.strip() for p in markdown_content.split('\\n\\n') if p.strip()]
   documents = [Document(page_content=paragraph) for paragraph in paragraphs]

Chaque double saut de ligne est interpr√©t√© comme une s√©paration logique entre les id√©es ou blocs de contenu. Cette segmentation est cruciale pour que les embeddings soient coh√©rents et repr√©sentatifs du contenu.

√âtape 5 : G√©n√©ration des Embeddings
-----------------------------------

Cette √©tape est centrale : elle convertit le texte en vecteurs num√©riques √† l‚Äôaide d‚Äôun mod√®le d‚Äôembedding compatible avec LangChain. Ces vecteurs sont ensuite stock√©s dans une base Chroma persistante.

.. code-block:: python
   :linenos:

   embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
   
   vecdb = Chroma.from_documents(
       documents=documents,
       embedding=embeddings,
       persist_directory="philo_db",
       collection_name="rag-chroma"
   )
   vecdb.persist()

Le mod√®le utilis√© ici, `mxbai-embed-large:latest`, encode chaque paragraphe en un vecteur dense de 1024 dimensions. Ces vecteurs sont ensuite index√©s et sauvegard√©s localement dans un dossier nomm√© `philo_db`. La collection `rag-chroma` permet de regrouper les documents selon un m√™me th√®me ou usage.

R√©sultats
---------

√Ä l'issue de ce processus, une base vectorielle est constitu√©e √† partir du contenu textuel extrait.

.. code-block:: text

   Op√©ration termin√©e avec succ√®s:
   - 914 paragraphes trait√©s
   - Base vectorielle sauvegard√©e dans: philo_db

Cette base peut d√©sormais √™tre utilis√©e pour la recherche s√©mantique, la d√©tection de plagiat ou l‚Äôimpl√©mentation d‚Äôun syst√®me RAG (Retrieval-Augmented Generation).

Notes Techniques
----------------

- **Format des embeddings** : chaque paragraphe est transform√© en un vecteur de 1024 dimensions, ce qui garantit une bonne expressivit√© s√©mantique.
- **Taille moyenne des paragraphes** : entre 150 et 300 mots, ce qui est optimal pour les mod√®les d‚Äôembedding modernes.
- **M√©tadonn√©es** : il est possible d‚Äôajouter des m√©tadonn√©es √† chaque `Document` (par exemple la langue, l‚Äôorigine du fichier, la section du document, etc.) pour des filtres ou recherches avanc√©es.

Conclusion
----------

Ce guide constitue une base robuste pour cr√©er une base vectorielle √† partir de documents PDF multilingues. Il est facilement extensible pour inclure plus de fichiers, enrichir les m√©tadonn√©es ou int√©grer des syst√®mes de recherche s√©mantique avanc√©e.




Travaux Futurs
==============

1. Am√©liorer les mod√®les en utilisant plus de donn√©es.
2. √âtendre la classification pour inclure d'autres comportements (vapoter, boire, etc.).

Conclusion
==========

Ce projet d√©montre la puissance de *MediaPipe* et *TensorFlow* pour r√©soudre des probl√®mes critiques li√©s √† la s√©curit√© et au bien-√™tre. L'int√©gration de ces outils offre une solution robuste¬†et¬†extensible.
