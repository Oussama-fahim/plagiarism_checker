================================================
Documentation du projet IA de plagiarism checker
================================================

Bienvenue dans la documentation du projet D√©tection de plagiat par Intelligence Artificielle √† l'aide de RAG et d'analyse s√©mantique avanc√©e.Ce document d√©taille les m√©thodologies, les outils utilis√©s, ainsi que les r√©sultats obtenus pour identifier les cas de plagiat, qu‚Äôils soient exacts, paraphras√©s ou s√©mantiquement similaires.L‚Äôobjectif de ce projet est de d√©velopper un syst√®me de d√©tection robuste en combinant plusieurs approches

*Table des mati√®res*

  - introduction
  - objectifs du projet
  - installation
  - pipeline 
  - creation d'une base de donn√©s vectorielle a partir de llama_parse 
  - application des approches (recherche hybride) 
  - cr√©ation d'une interface streamlit 
  - r√©sultat
  - Travaux Futurs
  - conclusion

.. toctree::
   :maxdepth: 2
   :hidden:
   
   documentation

.. raw:: html

   <div class="sidebar">
   <div class="toctree-wrapper">
   <h2>Table des mati√®res</h2>
   {{ toctree(maxdepth=2) }}
   </div>
   </div>


.. AI Plagiarism Sentinel Pro documentation master file

==============================================
AI Plagiarism Sentinel Pro ‚Äì D√©tection avanc√©e
==============================================

Introduction
============

La d√©tection automatique du plagiat est devenue un enjeu crucial dans le monde acad√©mique et professionnel, o√π la v√©rification de l'originalit√© d'un contenu est essentielle √† la cr√©dibilit√© intellectuelle et √† l‚Äôint√©grit√© des travaux. 

Ce projet propose une solution intelligente et robuste de d√©tection de plagiat en temps r√©el, exploitant les derni√®res avanc√©es en **traitement du langage naturel (NLP)**, **recherche vectorielle**, et **mod√®les de similarit√© s√©mantique multilingue**.

En s‚Äôappuyant sur une combinaison d‚Äô**embeddings puissants**, de **moteurs de recherche hybride**, de **mod√®les cross-encoders**, ainsi que d‚Äôune **analyse stylistique avanc√©e**, cette plateforme permet de d√©tecter diff√©rents types de plagiat : copie exacte, paraphrase, similitude conceptuelle, ou correspondance multilingue.

Le tout est encapsul√© dans une interface interactive d√©velopp√©e avec **Streamlit**, offrant √† l'utilisateur une exp√©rience fluide, visuelle, et totalement explicable.

Objectifs du projet
===================

- **Fournir un syst√®me de d√©tection de plagiat automatis√© et intelligent** :
  
  - Bas√© sur une base vectorielle *Chroma* enrichie par des embeddings (*Ollama embeddings*).
  - Capable d‚Äôidentifier non seulement des copies exactes, mais aussi des paraphrases et similitudes conceptuelles, y compris entre langues diff√©rentes (fran√ßais/anglais).

- **D√©montrer l‚Äôutilisation combin√©e de technologies modernes** :
  
  - üß† *LangChain*, *TF-IDF*, *Cross-Encoder* (MS-MARCO) pour l‚Äôanalyse s√©mantique.
  - üìö *SpaCy* et *textstat* pour l‚Äôanalyse stylistique, lisibilit√©, structure grammaticale, diversit√© lexicale.
  - üìä *Streamlit*, *Plotly*, *WordCloud*, *Pyvis* pour la visualisation avanc√©e et l‚Äôexplicabilit√©.

- **Assurer une analyse compl√®te et interpr√©table** :
  
  - G√©n√©ration d‚Äôun rapport technique JSON d√©taill√©.
  - R√©sum√© ex√©cutif avec score global, alertes de plagiat, et recommandations.
  - Visualisation des correspondances (r√©seaux interactifs, nuages de mots, graphiques).

- **Garantir une exp√©rience utilisateur professionnelle** :
  
  - Interface responsive avec personnalisation CSS.
  - Chargement de texte via saisie directe, fichiers (.pdf, .docx‚Ä¶), ou URL.
  - T√©l√©chargement dynamique de rapports avec score et diagnostic.




Installation
============

Les biblioth√®ques suivantes sont n√©cessaires pour le projet :

1. **os** : Manipulation des fichiers et chemins.
2. **pickle** : S√©rialisation et sauvegarde des objets Python.
3. **cv2** : Traitement d'images avec OpenCV.
4. **numpy** : Manipulation de matrices et calculs num√©riques.
5. **streamlit** : Interface web interactive pour l‚Äôutilisateur.
6. **langchain.vectorstores** : Gestion des bases vectorielles avec Chroma.
7. **langchain_community.embeddings** : G√©n√©ration des embeddings avec Ollama.
8. **langdetect** : D√©tection automatique de la langue d‚Äôun texte.
9. **typing** : Annotations de types (`List`, `Dict`, etc.).
10. **time** : Gestion du temps (dur√©e d‚Äôanalyse, timestamps).
11. **pandas** : Manipulation et affichage de tableaux de donn√©es.
12. **difflib** : Comparaison de cha√Ænes pour la similarit√© exacte.
13. **matplotlib.pyplot** : Visualisation de donn√©es.
14. **plotly.express** : Graphiques interactifs (camemberts, barres).
15. **collections.defaultdict** : Regroupement d‚Äô√©l√©ments similaires.
16. **re** : Expressions r√©guli√®res pour le nettoyage de texte.
17. **hashlib** : Hachage des textes pour la d√©tection exacte.
18. **sentence_transformers** : Calcul avanc√© de similarit√© s√©mantique (CrossEncoder).
19. **sklearn.feature_extraction.text** : TF-IDF vectorisation.
20. **sklearn.metrics.pairwise** : Similarit√© cosinus.
21. **PIL.Image** : Chargement et affichage d‚Äôimages.
22. **requests** : Requ√™te HTTP pour charger des images ou contenus.
23. **io.BytesIO** : Manipulation de contenu binaire.
24. **json** : S√©rialisation JSON pour les rapports.
25. **networkx** : Cr√©ation de graphes de similarit√©.
26. **pyvis.network** : Visualisation interactive de r√©seaux.
27. **tempfile** : Cr√©ation de fichiers temporaires.
28. **spacy** : Analyse grammaticale et entit√©s nomm√©es.
29. **wordcloud.WordCloud** : Nuage de mots bas√© sur les correspondances.
30. **textstat** : Analyse de lisibilit√©.
31. **streamlit.components.v1.html** : Affichage de composants HTML personnalis√©s.
32. **docx2txt** : Extraction de texte depuis fichiers Word.
33. **PyPDF2** : Extraction de texte depuis fichiers PDF.
34. **base64** : Encodage d‚Äôimages pour l‚Äôaffichage CSS.
35. **annotated_text** : Mise en √©vidence de texte dans Streamlit.
36. **st_aggrid** : Tableaux interactifs dans Streamlit.
37. **ollama** : Requ√™tes vers un mod√®le de langage local.

.. code-block:: python

   import os
   import pickle
   import cv2
   import numpy as np
   import streamlit as st
   import time
   import pandas as pd
   import matplotlib.pyplot as plt
   import plotly.express as px
   import re
   import hashlib
   import json
   import tempfile
   import requests
   import base64
   import docx2txt
   import PyPDF2
   import networkx as nx
   from PIL import Image
   from io import BytesIO
   from difflib import SequenceMatcher
   from collections import defaultdict
   from typing import List, Dict, Any, Tuple
   from sentence_transformers import CrossEncoder
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.metrics.pairwise import cosine_similarity
   from langchain.vectorstores import Chroma
   from langchain_community.embeddings import OllamaEmbeddings
   from wordcloud import WordCloud
   import spacy
   import textstat
   from streamlit.components.v1 import html
   from annotated_text import annotated_text
   from st_aggrid import AgGrid
   import ollama


pipeline
========
.. list-table::
   :widths: 200 200
   :align: center

   * - .. image:: image/1.png
         :alt: PIPLINE 
         :width: 700px
     - .. image:: image/2.png
         :alt: Image 2
         :width: 700px

explication de pipline:
-----------------------
Phase 1: Pr√©paration de la Base Vectorielle
-------------------------------------------

.. list-table::
   :header-rows: 1
   :widths: 10 30 60

   * - √âtape
     - Outils/M√©thodes
     - Description
   * - 1. Extraction
     - LlamaParse (FR/EN)
     - Conversion des PDF/DOCX en Markdown propre
   * - 2. Nettoyage
     - Regex + Unicode Normalization
     - Suppression des en-t√™tes, pieds de page, caract√®res sp√©ciaux
   * - 3. D√©coupage
     - ``split('\\n\\n')``
     - S√©paration en paragraphes (1 paragraphe = 1 document)
   * - 4. Embeddings
     - OllamaEmbeddings (mxbai-embed-large)
     - Vectorisation des paragraphes
   * - 5. Stockage
     - Chroma DB
     - Indexation avec m√©tadonn√©es (source, langue)
   * - 6. Persistance
     - ``vecdb.persist()``
     - Sauvegarde locale dans ``philo_db``

Phase 2: Analyse de Plagiat (Frontend/Backend)
----------------------------------------------

.. list-table::
   :header-rows: 1
   :widths: 10 30 60

   * - √âtape
     - Outils/M√©thodes
     - Description
   * - 1. Input Utilisateur
     - Streamlit (``file_uploader``/``text_area``)
     - Support pour texte direct, fichiers, ou URLs
   * - 2. Pr√©-processing
     - ``langdetect`` + ``spacy``
     - D√©tection de langue et nettoyage
   * - 3. Recherche Hybride
     - Combinaison de 3 m√©thodes:
     - 
   * - 
     - ‚Ä¢ **Exact Match** (MD5 + ``SequenceMatcher``)
     - D√©tection de copies mot-√†-mot
   * - 
     - ‚Ä¢ **Semantic Search** (Ollama + Cross-Encoder)
     - Similarit√© conceptuelle (seuil: 0.4-1.0)
   * - 
     - ‚Ä¢ **Multilingue** (Traduction via Llama3)
     - Comparaison FR‚ÜîEN
   * - 4. Post-Traitement
     - ``networkx`` + ``pyvis``
     - G√©n√©ration du r√©seau de similarit√©
   * - 5. Rapport
     - JSON + Streamlit
     - Export des r√©sultats d√©taill√©s


Cr√©ation d'une Base de Donn√©es Vectorielle avec LlamaParse
===========================================================

Ce guide fournit une proc√©dure compl√®te pour transformer un ou plusieurs fichiers PDF en une base de donn√©es vectorielle, utilisable notamment pour la d√©tection de similarit√© textuelle ou de plagiat. L'approche repose sur l'utilisation combin√©e de **LlamaParse** pour l'extraction intelligente de texte structur√© et de **LangChain** pour la vectorisation et la gestion des documents.

.. contents:: Sommaire
   :depth: 2
   :local:

√âtape 1 : Installation des D√©pendances
--------------------------------------

Cette premi√®re √©tape consiste √† importer l'ensemble des librairies n√©cessaires au bon fonctionnement du pipeline. 

.. code-block:: python
   :linenos:

   import os
   from llama_parse import LlamaParse
   from llama_parse.base import ResultType
   from langchain.text_splitter import RecursiveCharacterTextSplitter
   from langchain.vectorstores import Chroma
   from langchain.embeddings import HuggingFaceEmbeddings
   from langchain_core.documents import Document
   from llama_cloud_services.parse.utils import Language
   from langchain_community.embeddings.ollama import OllamaEmbeddings

Les modules import√©s remplissent des r√¥les sp√©cifiques :
- `llama_parse` permet d'extraire le contenu structur√© des PDF (en Markdown ici).
- `langchain` permet de g√©rer la transformation du texte en vecteurs ainsi que leur stockage dans une base.
- `OllamaEmbeddings` fournit un mod√®le d'embedding performant pour convertir du texte en vecteurs num√©riques.

√âtape 2 : Configuration de l'API LlamaParse
-------------------------------------------

Avant de lancer l'extraction, il est n√©cessaire de configurer LlamaParse avec une cl√© API valide. On peut √©galement sp√©cifier la langue du document pour am√©liorer la pr√©cision de l‚Äôanalyse.

.. code-block:: python
   :linenos:

   os.environ["LLAMA_CLOUD_API_KEY"] = "llx-a2C7FgYfP1hzX3pXuvtdaNmexAqsuRnJIJ2G6MjbBrfuS3QY"
   
   parser_fr = LlamaParse(
       result_type=ResultType.MD, 
       language=Language.FRENCH
   )
   parser_en = LlamaParse(
       result_type=ResultType.MD,
       language=Language.ENGLISH
   )

Deux parseurs sont initialis√©s ici : un pour les documents en fran√ßais et un autre pour ceux en anglais. Le format de sortie s√©lectionn√© est le Markdown (`ResultType.MD`), ce qui permet de conserver la structure logique du document original (titres, paragraphes, listes, etc.).

√âtape 3 : Extraction du Contenu PDF
-----------------------------------

On proc√®de ensuite √† l‚Äôextraction effective du contenu des fichiers PDF. LlamaParse utilisant des appels asynchrones, l‚Äôenvironnement doit √™tre adapt√© pour g√©rer cela correctement.

.. code-block:: python
   :linenos:

   import nest_asyncio
   nest_asyncio.apply()

   pdf_files = [("philosophie.pdf", parser_fr)]
   
   with open("plagia_data.md", 'w', encoding='utf-8') as f:
       for file_name, parser in pdf_files:
           print(f"Traitement de {file_name}...")
           documents = parser.load_data(file_name)
           f.write(f"# Contenu extrait de : {file_name}\\n\\n")
           for doc in documents:
               f.write(doc.text + "\\n\\n")

Chaque fichier est trait√© ind√©pendamment. Le texte extrait est structur√© et stock√© dans un fichier Markdown interm√©diaire (`plagia_data.md`). Cela facilite les traitements ult√©rieurs, notamment pour la segmentation en paragraphes ou sections.

√âtape 4 : Pr√©paration des Donn√©es
---------------------------------

Une fois le contenu extrait, il est lu depuis le fichier Markdown et segment√© en paragraphes. Ces derniers seront convertis en objets `Document`, reconnus par LangChain.

.. code-block:: python
   :linenos:

   with open("plagia_data.md", encoding='utf-8') as f:
       markdown_content = f.read()
   
   paragraphs = [p.strip() for p in markdown_content.split('\\n\\n') if p.strip()]
   documents = [Document(page_content=paragraph) for paragraph in paragraphs]

Chaque double saut de ligne est interpr√©t√© comme une s√©paration logique entre les id√©es ou blocs de contenu. Cette segmentation est cruciale pour que les embeddings soient coh√©rents et repr√©sentatifs du contenu.

√âtape 5 : G√©n√©ration des Embeddings
-----------------------------------

Cette √©tape est centrale : elle convertit le texte en vecteurs num√©riques √† l‚Äôaide d‚Äôun mod√®le d‚Äôembedding compatible avec LangChain. Ces vecteurs sont ensuite stock√©s dans une base Chroma persistante.

.. code-block:: python
   :linenos:

   embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
   
   vecdb = Chroma.from_documents(
       documents=documents,
       embedding=embeddings,
       persist_directory="philo_db",
       collection_name="rag-chroma"
   )
   vecdb.persist()

Le mod√®le utilis√© ici, `mxbai-embed-large:latest`, encode chaque paragraphe en un vecteur dense de 1024 dimensions. Ces vecteurs sont ensuite index√©s et sauvegard√©s localement dans un dossier nomm√© `philo_db`. La collection `rag-chroma` permet de regrouper les documents selon un m√™me th√®me ou usage.

R√©sultats
---------

√Ä l'issue de ce processus, une base vectorielle est constitu√©e √† partir du contenu textuel extrait.

.. code-block:: text

   Op√©ration termin√©e avec succ√®s:
   - 914 paragraphes trait√©s
   - Base vectorielle sauvegard√©e dans: philo_db

Cette base peut d√©sormais √™tre utilis√©e pour la recherche s√©mantique, la d√©tection de plagiat ou l‚Äôimpl√©mentation d‚Äôun syst√®me RAG (Retrieval-Augmented Generation).

Notes Techniques
----------------

- **Format des embeddings** : chaque paragraphe est transform√© en un vecteur de 1024 dimensions, ce qui garantit une bonne expressivit√© s√©mantique.
- **Taille moyenne des paragraphes** : entre 150 et 300 mots, ce qui est optimal pour les mod√®les d‚Äôembedding modernes.
- **M√©tadonn√©es** : il est possible d‚Äôajouter des m√©tadonn√©es √† chaque `Document` (par exemple la langue, l‚Äôorigine du fichier, la section du document, etc.) pour des filtres ou recherches avanc√©es.

Conclusion
----------

Ce guide constitue une base robuste pour cr√©er une base vectorielle √† partir de documents PDF multilingues. Il est facilement extensible pour inclure plus de fichiers, enrichir les m√©tadonn√©es ou int√©grer des syst√®mes de recherche s√©mantique avanc√©e.



Application des Approches de Recherche Hybride
=============================================

.. contents:: 
   :depth: 3
   :local:

Introduction
------------
La recherche hybride combine plusieurs techniques de similarit√© textuelle pour d√©tecter le plagiat √† diff√©rents niveaux :

1. **Recherche exacte** : D√©tection de copies mot-√†-mot
2. **Similarit√© s√©mantique** : Identification des paraphrases
3. **Analyse multilingue** : Comparaison entre langues (FR‚ÜîEN)

Architecture Principale
----------------------


.. image:: image/3.png
   :alt: architecture de la recherche hybride
   :width: 400px


Fonctions Cl√©s
--------------

check_exact_match()
~~~~~~~~~~~~~~~~~~~
.. code-block:: python
   :linenos:
   :emphasize-lines: 3-5,12-15

   def check_exact_match(input_text: str, dataset: List[str]) -> List[Tuple[str, float]]:
       """V√©rifie les correspondances exactes avec normalisation avanc√©e"""
       def normalize(text):
           text = re.sub(r'[^\w\s]', '', text.strip().lower())
           return re.sub(r'\s+', ' ', text)
       
       normalized_input = normalize(input_text)
       input_hash = hashlib.md5(normalized_input.encode('utf-8')).hexdigest()
       
       for doc in dataset:
           normalized_doc = normalize(doc)
           doc_hash = hashlib.md5(normalized_doc.encode('utf-8')).hexdigest()
           
           if input_hash == doc_hash:  # Match exact
               return [(doc, 1.0)]
           # Similarit√© textuelle avec SequenceMatcher
           match_ratio = SequenceMatcher(None, normalized_input, normalized_doc).ratio()

**Explication** : 

Cette fonction impl√©mente la premi√®re couche de la recherche hybride :

1. Normalisation du texte (minuscules, suppression ponctuation)
2. Hashing MD5 pour les correspondances exactes
3. ``SequenceMatcher`` pour les similarit√©s textuelles (>70%)
4. D√©tection de segments longs (fen√™tres de 8 mots)

translate_text()
~~~~~~~~~~~~~~~~
.. code-block:: python
   :linenos:

   @st.cache_data(ttl=3600)
   def translate_text(text: str, target_lang: str) -> str:
       """Traduction intelligente avec gestion des erreurs"""
       try:
           if len(text) < 50:  # Ne pas traduire les textes trop courts
               return text
               
           response = ollama.chat(
               model="llama3.1",
               messages=[{
                   "role": "system",
                   "content": f"Traduis ce texte en {target_lang}..."
               }],
               options={'temperature': 0.1}
           )
           return response["message"]["content"]

**R√¥le** :  

Permet la composante multilingue de la recherche hybride :

- Utilise Ollama/Llama3 pour les traductions FR‚ÜîEN
- Cache les r√©sultats pour 1 heure (optimisation performance)
- G√®re les textes courts (ne traduit pas en dessous de 50 caract√®res)

calculate_similarity()
~~~~~~~~~~~~~~~~~~~~~~
.. code-block:: python
   :linenos:

   def calculate_similarity(text1: str, text2: str) -> float:
       """Calcule la similarit√© combin√©e TF-IDF + Cross-Encoder"""
       # Similarit√© lexicale (TF-IDF)
       vectors = tfidf_vectorizer.transform([text1, text2])
       tfidf_sim = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
       
       # Similarit√© s√©mantique (Cross-Encoder)
       cross_score = cross_encoder.predict([[text1, text2]])[0]
       
       return (cross_score * 0.7) + (tfidf_sim * 0.3)  # Combinaison pond√©r√©e

**Fonctionnement** :  

Coeur de l'approche hybride :

1. **TF-IDF** : Similarit√© surfacelle (mots-cl√©s, n-grams)
2. **Cross-Encoder** : Compr√©hension s√©mantique profonde
3. Pond√©ration : 70% s√©mantique + 30% lexicale

hybrid_search()
~~~~~~~~~~~~~~~
.. code-block:: python
   :linenos:
   :emphasize-lines: 8-9,15-17,25-27

   def hybrid_search(query: str, dataset: List[str], top_k: int = 10) -> List[Dict[str, Any]]:
       """Recherche hybride multilingue"""
       # 1. D√©tection langue
       query_lang = detect(query) if len(query) > 20 else 'en'
       
       # 2. Recherche exacte
       exact_matches = check_exact_match(query, dataset)
       if exact_matches:
           return [{"match_type": "exact", ...}]
       
       # 3. Recherche vectorielle
       vector_results = vecdb.similarity_search_with_score(query, k=top_k*2)
       
       # 4. Recherche multilingue
       if query_lang == 'fr':
           translated_query = translate_text(query, 'en')
           translated_results = vecdb.similarity_search_with_score(translated_query, k=top_k)
       
       # Combinaison et filtrage
       all_results = [...]
       return sorted(all_results, key=lambda x: x["combined_score"], reverse=True)[:top_k]

**Workflow** :

1. Orchestre les diff√©rentes m√©thodes de recherche
2. Combine les r√©sultats natifs et traduits
3. Applique des p√©nalit√©s aux r√©sultats traduits (-10%)
4. Trie par score combin√©

analyze_ideas()
~~~~~~~~~~~~~~~
.. code-block:: python
   :linenos:

   def analyze_ideas(input_text: str, matches: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
       """Analyse des similarit√©s conceptuelles entre phrases"""
       ideas = []
       sentences = [s.strip() for s in re.split(r'[.!?]', input_text) if len(s.strip().split()) > 5]
       
       for match in matches:
           if match["combined_score"] < 0.4:  # Seuil minimal
               continue
           match_sentences = [s.strip() for s in re.split(r'[.!?]', match["content"]) if len(s.strip().split()) > 5]
           
           for sent in sentences:
               for match_sent in match_sentences:
                   sim_score = calculate_similarity(sent, match_sent)
                   if sim_score > 0.5:  # Seuil id√©e similaire
                       ideas.append({
                           "source_sentence": sent,
                           "matched_sentence": match_sent,
                           "similarity": sim_score
                       })

**Objectif** :  
D√©tecte les plagiat conceptuel en :

- D√©coupant le texte en phrases
- Comparant chaque paire de phrases
- Gardant les matches >50% de similarit√©
- Groupant les id√©es similaires

Visualisation des R√©sultats
---------------------------

create_similarity_network()
~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. code-block:: python
   :linenos:

   def create_similarity_network(matches):
       """Cr√©e un r√©seau de similarit√© interactif"""
       G = nx.Graph()
       for i, match in enumerate(matches):
           G.add_node(f"Source_{i}", color='blue')
           G.add_node(match['source'], color='red')
           G.add_edge(f"Source_{i}", match['source'], weight=match['score'])
       
       net = Network(height="500px")
       net.from_nx(G)
       return net

**R√¥le** :  
G√©n√®re une visualisation interactive des connexions entre :

- Le texte source (n≈ìuds bleus)
- Les documents trouv√©s (n≈ìuds rouges)
- Les ar√™tes pond√©r√©es par le score de similarit√©

Conclusion
----------
Cette approche hybride combine :

- **Pr√©cision** : D√©tection des copies exactes
- **Nuance** : Compr√©hension s√©mantique
- **Couverure** : Analyse multilingue
- **Transparence** : Visualisations explicatives

cr√©ation d'une interface streamlit 
==================================

Cette partie d√©taille la conception et l'impl√©mentation d'une interface Streamlit compl√®te pour une application de d√©tection de plagiat AI-powered.

.. contents:: Table des Mati√®res
   :depth: 3
   :local:

Introduction
------------

L'interface Streamlit a √©t√© con√ßue pour offrir une exp√©rience utilisateur riche avec :

- Un dashboard interactif
- Des visualisations de donn√©es avanc√©es
- Une analyse en temps r√©el
- Un design responsive et moderne


Configuration Initiale
----------------------

.. code-block:: python

    import streamlit as st
    st.set_page_config(
        layout="wide", 
        page_title="üîç AI Plagiarism Sentinel Pro", 
        page_icon="üîç"
    )

Explications :
~~~~~~~~~~~~~~
- ``layout="wide"`` permet d'utiliser toute la largeur de l'√©cran
- Personnalisation du titre et de l'ic√¥ne pour une identit√© visuelle

Initialisation des Mod√®les
--------------------------

.. code-block:: python

    @st.cache_resource(show_spinner=False)
    def initialize_system():
        # Initialisation des embeddings
        embeddings = OllamaEmbeddings(
            model="mxbai-embed-large:latest",
            temperature=0.01,
            top_k=50
        )
        
        # Initialisation de la base vectorielle
        vecdb = Chroma(
            persist_directory="philo_db",
            embedding_function=embeddings,
            collection_name="rag-chroma"
        )

Explications :
~~~~~~~~~~~~~~
- ``@st.cache_resource`` optimise les performances en cachant les ressources initialis√©es
- La fonction charge les mod√®les NLP et la base de donn√©es vectorielle

--------------------
Interface Utilisateur
--------------------

En-t√™te Personnalis√©
--------------------

.. code-block:: python

    def load_assets():
        try:
            response = requests.get("https://images.unsplash.com/photo-1620712943543-bcc4688e7485")
            banner = Image.open(BytesIO(response.content))
            return banner
        except:
            return None

    banner_image = load_assets()
    if banner_image:
        st.image(banner_image, use_column_width=True)
    else:
        st.markdown("""
        <div class="header">
            <h1>üîç AI Plagiarism Sentinel Pro</h1>
        </div>
        """, unsafe_allow_html=True)

Explications :
~~~~~~~~~~~~~~
- T√©l√©chargement dynamique d'une banni√®re
- Fallback sur un en-t√™te HTML si l'image n'est pas disponible

Sidebar Configurable
--------------------

.. code-block:: python

    with st.sidebar:
        st.title("‚öôÔ∏è Param√®tres Experts")
        
        with st.expander("üîç Options de Recherche", expanded=True):
            analysis_mode = st.selectbox(
                "Mode d'analyse",
                ["DeepScan Pro", "Rapide", "Manuel Expert"]
            )
            
            sensitivity = st.slider(
                "Niveau de sensibilit√©",
                1, 10, 8
            )

Explications :
~~~~~~~~~~~~~~
- Organisation des contr√¥les dans des expanders
- Utilisation de widgets Streamlit vari√©s (selectbox, slider)

Zone de Saisie Multimode
------------------------

.. code-block:: python

    input_method = st.radio(
        "Source d'entr√©e",
        ["üìù Texte direct", "üìÇ Fichier", "üåê URL"],
        horizontal=True
    )
    
    if input_method == "üìÇ Fichier":
        uploaded_file = st.file_uploader(
            "T√©l√©versez un document",
            type=["txt", "pdf", "docx"]
        )

Explications :
~~~~~~~~~~~~~~
- Interface unifi√©e pour diff√©rentes m√©thodes de saisie
- Traitement sp√©cifique pour chaque type d'entr√©e

Visualisations Avanc√©es
-----------------------

Cartes de R√©sultats
-------------------

.. code-block:: python

    def display_match_card(match):
        with st.container():
            st.markdown(f"""
            <div class="{'exact-match' if match['match_type'] == 'exact' else 'partial-match'}">
                <h3>{match['match_type'].capitalize()} - Score: {match['combined_score']*100:.1f}%</h3>
                <p><strong>Source:</strong> {match['metadata'].get('source', 'Inconnue')}</p>
            </div>
            """, unsafe_allow_html=True)

Explications :
~~~~~~~~~~~~~~
- Utilisation de HTML/CSS pour des cartes stylis√©es
- Classes CSS dynamiques en fonction du type de correspondance

R√©seau de Similarit√©
--------------------

.. code-block:: python

    def create_similarity_network(matches):
        G = nx.Graph()
        for i, match in enumerate(matches):
            G.add_node(f"Source_{i}", size=15, color='blue')
            G.add_edge(f"Source_{i}", match['metadata'].get('source', f"Doc_{i}"), 
                      weight=match['combined_score'])
        
        net = Network(height="500px", width="100%")
        net.from_nx(G)
        return net

Explications :
~~~~~~~~~~~~~~
- Utilisation de NetworkX pour la cr√©ation du graphe
- Int√©gration avec PyVis pour le rendu interactif

---------------------
Gestion des Donn√©es
-------------------

Cache et Performance
--------------------

.. code-block:: python

    @st.cache_data(ttl=3600)
    def translate_text(text: str, target_lang: str) -> str:
        # Fonction de traduction
        return translated_text

Explications :
~~~~~~~~~~~~~~
- ``@st.cache_data`` pour cacher les r√©sultats co√ªteux
- TTL (Time-To-Live) de 1 heure pour les traductions

Traitement des Fichiers
-----------------------

.. code-block:: python

    if uploaded_file.type == "application/pdf":
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = "\n".join([page.extract_text() for page in pdf_reader.pages])
    elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        text = docx2txt.process(uploaded_file)

Explications :
~~~~~~~~~~~~~~
- Support multi-format (PDF, DOCX, etc.)
- Extraction robuste du texte

------------------
Design Avanc√©
-------------

CSS Personnalis√©
----------------

.. code-block:: python

    def apply_custom_css():
        css = """
        <style>
            .header {
                background: linear-gradient(135deg, #434343 0%, #000000 100%);
                color: white;
                padding: 2rem;
                border-radius: 10px;
            }
            .exact-match { 
                border-left: 6px solid #ef4444; 
                background-color: rgba(239, 68, 68, 0.05);
            }
        </style>
        """
        st.markdown(css, unsafe_allow_html=True)

Explications :
~~~~~~~~~~~~~~
- Styles CSS int√©gr√©s directement dans Streamlit
- Utilisation de gradients et d'effets modernes

Mise en Page
------------

.. code-block:: python

    col1, col2 = st.columns(2)
    with col1:
        st.metric("Score maximal", f"{score:.1f}%")
    with col2:
        st.metric("Correspondances", count)

Explications :
~~~~~~~~~~~~~~
- Layout multi-colonnes pour une organisation optimale
- Widgets de m√©triques pour les KPI

--------------------
Fonctionnalit√©s Avanc√©es
------------------------

Onglets Interactifs
-------------------

.. code-block:: python

    tab1, tab2 = st.tabs(["üìä Dashboard", "üîç Correspondances"])
    with tab1:
        st.plotly_chart(fig)
    with tab2:
        for match in matches:
            display_match_card(match)

Explications :
~~~~~~~~~~~~~~
- Navigation par onglets pour organiser le contenu
- Contenu dynamique dans chaque onglet

G√©n√©ration de Rapports
----------------------

.. code-block:: python

    def generate_full_report(results):
        return json.dumps({
            "metadata": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_matches": len(results.get('all_matches', []))
            },
            "results": {
                "highest_score": results.get('highest_score', 0),
            }
        }, indent=2)

Explications :
~~~~~~~~~~~~~~
- Format JSON structur√©
- T√©l√©chargement direct via Streamlit

--------------------
Conclusion
----------

Cette interface Streamlit combine :
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Des composants UI riches
- Des visualisations interactives
- Une gestion efficace des donn√©es
- Un design moderne personnalisable

Les techniques pr√©sent√©es peuvent √™tre adapt√©es pour tout type d'application data-centric.

resultas
========



Travaux futurs
==============

Cette partie pr√©sente les am√©liorations potentielles pour la future version du syst√®me de d√©tection de plagiat.

1. Am√©liorations des Algorithmes
--------------------------------

1.1. Int√©gration de Mod√®les Multilingues Avanc√©s
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Ajout de mod√®les sp√©cialis√©s pour d'autres langues (espagnol, allemand, chinois)
- Impl√©mentation d'un syst√®me de d√©tection automatique de langue plus robuste
- Optimisation des traductions avec des mod√®les d√©di√©s (NLLB, DeepL)

1.2. Am√©lioration des Scores de Similarit√©
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Combinaison de plusieurs m√©triques (BERTScore, ROUGE, BLEU)
- Ajout d'un syst√®me de pond√©ration dynamique bas√© sur le contexte
- Int√©gration de mod√®les de similarit√© sp√©cifiques aux domaines (scientifique, juridique)

2. Fonctionnalit√©s Avanc√©es
---------------------------

2.1. Analyse Temporelle
~~~~~~~~~~~~~~~~~~~~~~~
- D√©tection des variations stylistiques dans le texte
- Identification des ajouts/modifications successifs
- Reconstruction de l'historique d'√©criture

2.2. D√©tection de Paraphrase Sophistiqu√©e
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Mod√®les sp√©cifiques pour identifier les paraphrases avanc√©es
- D√©tection des modifications structurelles (changement d'ordre des id√©es)
- Analyse des patterns de r√©√©criture

3. Interface Utilisateur
------------------------

3.1. Tableau de Bord Analytique
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Visualisations interactives des r√©sultats
- Comparaison avec les soumissions pr√©c√©dentes
- Suivi des am√©liorations dans les r√©visions

3.2. Outils d'Aide √† la R√©√©criture
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Suggestions de reformulation originales
- G√©n√©rateur de citations automatiques
- Identification des passages √† risque

4. Infrastructure Technique
---------------------------

4.1. Optimisation des Performances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Impl√©mentation d'un syst√®me de cache distribu√©
- Pr√©traitement asynchrone des documents
- Indexation incr√©mentielle

4.2. Extension des Bases de R√©f√©rence
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- Int√©gration de nouvelles sources acad√©miques
- Connexion aux bases de donn√©es ouvertes
- Mise √† jour automatique du corpus de r√©f√©rence

5. Int√©grations Syst√®me
-----------------------

5.1. API Universelle
~~~~~~~~~~~~~~~~~~~~
- D√©veloppement d'une API RESTful compl√®te
- Int√©gration avec les LMS (Moodle, Canvas)
- Connecteurs pour les outils d'√©dition (Word, Google Docs)

5.2. Modules Sp√©cialis√©s
~~~~~~~~~~~~~~~~~~~~~~~~
- Version pour l'√©dition scientifique
- Module d√©di√© √† l'√©ducation
- Solution pour les √©diteurs professionnels

Perspectives √† Long Terme
-------------------------

- Analyse multimodale (texte + images + formules)
- D√©tection cross-m√©dia (vid√©os, podcasts)
- Syst√®me pr√©dictif de risque de plagiat
- Blockchain pour la tra√ßabilit√© des sources

Ces am√©liorations permettront de positionner l'outil comme une solution compl√®te de v√©rification d'int√©grit√© acad√©mique et professionnelle.


Conclusion
==========

Apr√®s la r√©alisation de ce projet AI Plagiarism Sentinel Pro, plusieurs constats importants peuvent √™tre tir√©s :

1. **Efficacit√© de d√©tection** : 
   Le syst√®me combine avec succ√®s diff√©rentes approches (correspondance exacte, analyse s√©mantique, similarit√© conceptuelle) pour offrir une d√©tection de plagiat multi-niveaux tr√®s performante.

2. **Innovation technologique** :
   L'utilisation combin√©e de mod√®les de langue (Ollama), d'embeddings vectoriels et de techniques traditionnelles (TF-IDF) permet une analyse √† la fois profonde et rapide.

3. **Polyvalence linguistique** :
   La capacit√© √† traiter plusieurs langues (notamment fran√ßais et anglais) et √† identifier des similarit√©s translinguistiques constitue un atout majeur.

4. **Analyse stylistique** :
   Les fonctionnalit√©s d'analyse d'√©criture vont au-del√† de la simple d√©tection de plagiat, offrant des insights pr√©cieux sur le style et la qualit√© r√©dactionnelle.

5. **Interface intuitive** :
   Le dashboard Streamlit propose une exp√©rience utilisateur riche tout en restant accessible, avec des visualisations claires et des rapports d√©taill√©s.


