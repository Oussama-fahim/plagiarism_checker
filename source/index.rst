================================================
Documentation du projet IA de plagiarism checker
================================================

Bienvenue dans la documentation du projet D√©tection de plagiat par Intelligence Artificielle √† l'aide de RAG et d'analyse s√©mantique avanc√©e.Ce document d√©taille les m√©thodologies, les outils utilis√©s, ainsi que les r√©sultats obtenus pour identifier les cas de plagiat, qu‚Äôils soient exacts, paraphras√©s ou s√©mantiquement similaires.L‚Äôobjectif de ce projet est de d√©velopper un syst√®me de d√©tection robuste en combinant plusieurs approches

*Table des mati√®res*

  - introduction
  - objectifs du projet
  - installation
  - pipeline 
  - creation d'une base de donn√©s a partir de llama_parse 
  - application des approches (recherche hybride)
  - visualisation des r√©sultat 
  - cr√©ation d'une interface streamlit 
  - Travaux Futurs
  - conclusion


.. AI Plagiarism Sentinel Pro documentation master file

==============================================
AI Plagiarism Sentinel Pro ‚Äì D√©tection avanc√©e
==============================================

Introduction
============

La d√©tection automatique du plagiat est devenue un enjeu crucial dans le monde acad√©mique et professionnel, o√π la v√©rification de l'originalit√© d'un contenu est essentielle √† la cr√©dibilit√© intellectuelle et √† l‚Äôint√©grit√© des travaux. 

Ce projet propose une solution intelligente et robuste de d√©tection de plagiat en temps r√©el, exploitant les derni√®res avanc√©es en **traitement du langage naturel (NLP)**, **recherche vectorielle**, et **mod√®les de similarit√© s√©mantique multilingue**.

En s‚Äôappuyant sur une combinaison d‚Äô**embeddings puissants**, de **moteurs de recherche hybride**, de **mod√®les cross-encoders**, ainsi que d‚Äôune **analyse stylistique avanc√©e**, cette plateforme permet de d√©tecter diff√©rents types de plagiat : copie exacte, paraphrase, similitude conceptuelle, ou correspondance multilingue.

Le tout est encapsul√© dans une interface interactive d√©velopp√©e avec **Streamlit**, offrant √† l'utilisateur une exp√©rience fluide, visuelle, et totalement explicable.

Objectifs du projet
===================

- **Fournir un syst√®me de d√©tection de plagiat automatis√© et intelligent** :
  
  - Bas√© sur une base vectorielle *Chroma* enrichie par des embeddings (*Ollama embeddings*).
  - Capable d‚Äôidentifier non seulement des copies exactes, mais aussi des paraphrases et similitudes conceptuelles, y compris entre langues diff√©rentes (fran√ßais/anglais).

- **D√©montrer l‚Äôutilisation combin√©e de technologies modernes** :
  
  - üß† *LangChain*, *TF-IDF*, *Cross-Encoder* (MS-MARCO) pour l‚Äôanalyse s√©mantique.
  - üìö *SpaCy* et *textstat* pour l‚Äôanalyse stylistique, lisibilit√©, structure grammaticale, diversit√© lexicale.
  - üìä *Streamlit*, *Plotly*, *WordCloud*, *Pyvis* pour la visualisation avanc√©e et l‚Äôexplicabilit√©.

- **Assurer une analyse compl√®te et interpr√©table** :
  
  - G√©n√©ration d‚Äôun rapport technique JSON d√©taill√©.
  - R√©sum√© ex√©cutif avec score global, alertes de plagiat, et recommandations.
  - Visualisation des correspondances (r√©seaux interactifs, nuages de mots, graphiques).

- **Garantir une exp√©rience utilisateur professionnelle** :
  
  - Interface responsive avec personnalisation CSS.
  - Chargement de texte via saisie directe, fichiers (.pdf, .docx‚Ä¶), ou URL.
  - T√©l√©chargement dynamique de rapports avec score et diagnostic.




Installation
============

Les biblioth√®ques suivantes sont n√©cessaires pour le projet :

1. **os** : Manipulation des fichiers et chemins.
2. **pickle** : S√©rialisation et sauvegarde des objets Python.
3. **cv2** : Traitement d'images avec OpenCV.
4. **numpy** : Manipulation de matrices et calculs num√©riques.
5. **streamlit** : Interface web interactive pour l‚Äôutilisateur.
6. **langchain.vectorstores** : Gestion des bases vectorielles avec Chroma.
7. **langchain_community.embeddings** : G√©n√©ration des embeddings avec Ollama.
8. **langdetect** : D√©tection automatique de la langue d‚Äôun texte.
9. **typing** : Annotations de types (`List`, `Dict`, etc.).
10. **time** : Gestion du temps (dur√©e d‚Äôanalyse, timestamps).
11. **pandas** : Manipulation et affichage de tableaux de donn√©es.
12. **difflib** : Comparaison de cha√Ænes pour la similarit√© exacte.
13. **matplotlib.pyplot** : Visualisation de donn√©es.
14. **plotly.express** : Graphiques interactifs (camemberts, barres).
15. **collections.defaultdict** : Regroupement d‚Äô√©l√©ments similaires.
16. **re** : Expressions r√©guli√®res pour le nettoyage de texte.
17. **hashlib** : Hachage des textes pour la d√©tection exacte.
18. **sentence_transformers** : Calcul avanc√© de similarit√© s√©mantique (CrossEncoder).
19. **sklearn.feature_extraction.text** : TF-IDF vectorisation.
20. **sklearn.metrics.pairwise** : Similarit√© cosinus.
21. **PIL.Image** : Chargement et affichage d‚Äôimages.
22. **requests** : Requ√™te HTTP pour charger des images ou contenus.
23. **io.BytesIO** : Manipulation de contenu binaire.
24. **json** : S√©rialisation JSON pour les rapports.
25. **networkx** : Cr√©ation de graphes de similarit√©.
26. **pyvis.network** : Visualisation interactive de r√©seaux.
27. **tempfile** : Cr√©ation de fichiers temporaires.
28. **spacy** : Analyse grammaticale et entit√©s nomm√©es.
29. **wordcloud.WordCloud** : Nuage de mots bas√© sur les correspondances.
30. **textstat** : Analyse de lisibilit√©.
31. **streamlit.components.v1.html** : Affichage de composants HTML personnalis√©s.
32. **docx2txt** : Extraction de texte depuis fichiers Word.
33. **PyPDF2** : Extraction de texte depuis fichiers PDF.
34. **base64** : Encodage d‚Äôimages pour l‚Äôaffichage CSS.
35. **annotated_text** : Mise en √©vidence de texte dans Streamlit.
36. **st_aggrid** : Tableaux interactifs dans Streamlit.
37. **ollama** : Requ√™tes vers un mod√®le de langage local.

.. code-block:: python

   import os
   import pickle
   import cv2
   import numpy as np
   import streamlit as st
   import time
   import pandas as pd
   import matplotlib.pyplot as plt
   import plotly.express as px
   import re
   import hashlib
   import json
   import tempfile
   import requests
   import base64
   import docx2txt
   import PyPDF2
   import networkx as nx
   from PIL import Image
   from io import BytesIO
   from difflib import SequenceMatcher
   from collections import defaultdict
   from typing import List, Dict, Any, Tuple
   from sentence_transformers import CrossEncoder
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.metrics.pairwise import cosine_similarity
   from langchain.vectorstores import Chroma
   from langchain_community.embeddings import OllamaEmbeddings
   from wordcloud import WordCloud
   import spacy
   import textstat
   from streamlit.components.v1 import html
   from annotated_text import annotated_text
   from st_aggrid import AgGrid
   import ollama


D√©tection de la Fatigue
=======================

1. *Collecte des donn√©es* :
- T√©l√©charger et collecter le dataset depuis Kaggle en utilisant le site suivant : https://www.kaggle.com/datasets/ismailnasri20/driver-drowsiness-dataset-ddd    

- Organisation en deux dossiers :
     - *Drowsy* : Images de personnes somnolentes.
     - *Non Drowsy* : Images de personnes √©veill√©es.

.. code-block:: python

    path = r"C:\Users\n\Desktop\projet ia\data1\FATIGUE"
    suffix ="phot"

exemple de data :

.. list-table::
   :widths: 50 50
   :align: center

   * - .. image:: image/A0100.png
         :alt: Image 1
         :width: 300px
     - .. image:: image/a0103.png
         :alt: Image 2
         :width: 300px

2. *Analyse des landmarks faciaux avec MediaPipe* :
   - Utilisation de *MediaPipe FaceMesh* pour extraire les points cl√©s.

.. code-block:: python

   mp_face_mesh = mp.solutions.face_mesh
   face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.3, min_tracking_confidence=0.8)
   mp_drawing = mp.solutions.drawing_utils 
   drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

3. *Calcul des caract√©ristiques* :
   - EAR : Eye Aspect Ratio.
   - MAR : Mouth Aspect Ratio.
   
.. code-block:: python

  right_eye = [[33, 133], [160, 144], [159, 145], [158, 153]] # right eye landmark positions
  left_eye = [[263, 362], [387, 373], [386, 374], [385, 380]] # left eye landmark positions
  mouth = [[61, 291], [39, 181], [0, 17], [269, 405]] # mouth landmark coordinates

.. code-block:: python

  def distance(p1, p2):
      return (((p1[:2] - p2[:2])*2).sum())*0.5

  def eye_aspect_ratio(landmarks, eye):
      N1 = distance(landmarks[eye[1][0]], landmarks[eye[1][1]])
      N2 = distance(landmarks[eye[2][0]], landmarks[eye[2][1]])
      N3 = distance(landmarks[eye[3][0]], landmarks[eye[3][1]])
      D = distance(landmarks[eye[0][0]], landmarks[eye[0][1]])
      return (N1 + N2 + N3) / (3 * D)

  def eye_feature(landmarks):
      return (eye_aspect_ratio(landmarks, left_eye) + eye_aspect_ratio(landmarks, right_eye)) / 2

  def mouth_feature(landmarks):
      N1 = distance(landmarks[mouth[1][0]], landmarks[mouth[1][1]])
      N2 = distance(landmarks[mouth[2][0]], landmarks[mouth[2][1]])
      N3 = distance(landmarks[mouth[3][0]], landmarks[mouth[3][1]])
      D = distance(landmarks[mouth[0][0]], landmarks[mouth[0][1]])
      return (N1 + N2 + N3) / (3 * D)

4. *Extraction et sauvegarde* :

pour les images somnolentes
===========================

√âtape 1: extraction de caract√©ristiques
--------------------------------------
Le code suivant extrait les caract√©ristiques (ear et mar) des images somnolentes dans le jeu de donn√©es et les enregistre dans un fichier pickle :

.. code-block:: python

    drowsy_feats = [] 
    drowsy_path = os.path.join(path, "drowsy")

    # Check if directory exists
    if not os.path.exists(drowsy_path):
        print(f"Directory {drowsy_path} does not exist.")
    else:
        drowsy_list = os.listdir(drowsy_path)
        print(f"Total images in drowsy directory: {len(drowsy_list)}")

        for name in drowsy_list:
            image_path = os.path.join(drowsy_path, name)
            image = cv2.imread(image_path)
            
            # Check if image was loaded successfully
            if image is None:
                print(f"Could not read image {image_path}. Skipping.")
                continue

            # Flip and convert the image to RGB
            image_rgb = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)
            
            # Process the image with face mesh
            results = face_mesh.process(image_rgb)

            if results.multi_face_landmarks:
                landmarks_positions = []
                # assume that only face is present in the image
                for _, data_point in enumerate(results.multi_face_landmarks[0].landmark):
                    landmarks_positions.append([data_point.x, data_point.y, data_point.z]) # saving normalized landmark positions
                landmarks_positions = np.array(landmarks_positions)
                landmarks_positions[:, 0] *= image.shape[1]
                landmarks_positions[:, 1] *= image.shape[0]

                ear = eye_feature(landmarks_positions)
                mar = mouth_feature(landmarks_positions)
                drowsy_feats.append((ear, mar))
            else:
                continue

        # Convert features list to numpy array and save to a file
        drowsy_feats = np.array(drowsy_feats)
        output_path = os.path.join("./feats", f"{suffix}_mp_drowsy_feats.pkl")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, "wb") as fp:
            pickle.dump(drowsy_feats, fp)

        print(f"Feature extraction complete. Saved to {output_path}")

√âtape 2: Charger les caract√©ristiques extraites
----------------------------------------------

.. code-block:: python

    with open("./feats/phot_mp_drowsy_feats.pkl", "rb") as fp:
        drowsy_feats = pickle.load(fp)

pour les images non somnolentes
===============================     

√âtape 1 : Extraction de caract√©ristiques
----------------------------------------

Le code suivant extrait les caract√©ristiques (ear et mar) des images non somnolentes dans le jeu de donn√©es et les enregistre dans un fichier pickle :

.. code-block:: python

    not_drowsy_feats = [] 
    not_drowsy_path = os.path.join(path, "notdrowsy")

    # V√©rifier si le r√©pertoire existe
    if not os.path.exists(not_drowsy_path):
        print(f"Le r√©pertoire {not_drowsy_path} n'existe pas.")
    else:
        not_drowsy_list = os.listdir(not_drowsy_path)
        print(f"Total d'images dans le r√©pertoire notdrowsy : {len(not_drowsy_list)}")

        for name in not_drowsy_list:
            image_path = os.path.join(not_drowsy_path, name)
            image = cv2.imread(image_path)
            
            # V√©rifier si l'image a √©t√© charg√©e correctement
            if image is None:
                print(f"Impossible de lire l'image {image_path}. Passage √† l'image suivante.")
                continue

            # Retourner et convertir l'image en RGB
            image_rgb = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)
            
            # Traiter l'image avec le mesh du visage
            results = face_mesh.process(image_rgb)

            if results.multi_face_landmarks:
                landmarks_positions = []
                # Supposer qu'il n'y a qu'un seul visage dans l'image
                for _, data_point in enumerate(results.multi_face_landmarks[0].landmark):
                    landmarks_positions.append([data_point.x, data_point.y, data_point.z]) # Sauvegarder les positions des landmarks normalis√©es
                landmarks_positions = np.array(landmarks_positions)
                landmarks_positions[:, 0] *= image.shape[1]  # Mise √† l'√©chelle des coordonn√©es x
                landmarks_positions[:, 1] *= image.shape[0]  # Mise √† l'√©chelle des coordonn√©es y

                # Extraire les caract√©ristiques
                ear = eye_feature(landmarks_positions)
                mar = mouth_feature(landmarks_positions)
                not_drowsy_feats.append((ear, mar))
            else:
                continue

        # Convertir la liste de caract√©ristiques en un tableau numpy et l'enregistrer dans un fichier
        not_drowsy_feats = np.array(not_drowsy_feats)
        output_path = os.path.join("./feats", f"{suffix}_mp_not_drowsy_feats.pkl")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, "wb") as fp:
            pickle.dump(not_drowsy_feats, fp)

        print(f"L'extraction des caract√©ristiques est termin√©e. Sauvegard√© dans {output_path}")

√âtape 2 : Charger les caract√©ristiques extraites
------------------------------------------------

.. code-block:: python

    with open("./feats/phot_mp_not_drowsy_feats.pkl", "rb") as fp:
        non_drowsy_feats = pickle.load(fp)

5. *statistique de data* :

.. code-block:: python

   print(f"Drowsy Images: {drowsy_feats.shape[0]}")
   drowsy_ear = drowsy_feats[:, 0]
   print(f"EAR | Min, Median, Mean, Max, SD: [{drowsy_ear.min()}, {np.median(drowsy_ear)}, {drowsy_ear.mean()}, {drowsy_ear.max()}, {drowsy_ear.std()}]")
   drowsy_mar = drowsy_feats[:, 1]
   print(f"MAR | Min, Median, Mean, Max, SD: [{drowsy_mar.min()}, {np.median(drowsy_mar)}, {drowsy_mar.mean()}, {drowsy_mar.max()}, {drowsy_mar.std()}]")

Drowsy Images: 22348
EAR | Min, Median, Mean, Max, SD: [0.05643663213581103, 0.23440516640901327, 0.23769841002149675, 0.4788618089840052, 0.06175599084484693]
MAR | Min, Median, Mean, Max, SD: [0.1579104064072938, 0.27007593084743897, 0.29444085404221526, 0.852751604533097, 0.07479365878783618]

.. code-block:: python

   print(f"Non Drowsy Images: {non_drowsy_feats.shape[0]}")
   non_drowsy_ear = non_drowsy_feats[:, 0]
   print(f"EAR | Min, Median, Mean, Max, SD: [{non_drowsy_ear.min()}, {np.median(non_drowsy_ear)}, {non_drowsy_ear.mean()}, {non_drowsy_ear.max()}, {non_drowsy_ear.std()}]")
   non_drowsy_mar = non_drowsy_feats[:, 1]
   print(f"MAR | Min, Median, Mean, Max, SD: [{non_drowsy_mar.min()}, {np.median(non_drowsy_mar)}, {non_drowsy_mar.mean()}, {non_drowsy_mar.max()}, {non_drowsy_mar.std()}]")

Non Drowsy Images: 19445
EAR | Min, Median, Mean, Max, SD: [0.0960194509125116, 0.26370564454608236, 0.2704957278714779, 0.4394997191869294, 0.047188973064084226]
MAR | Min, Median, Mean, Max, SD: [0.139104718407629, 0.2955462164966127, 0.30543910382658035, 0.5770066727463391, 0.06818546886870354]

6. *Mod√©lisation et entra√Ænement* :

.. code-block:: python

    s = 192
    np.random.seed(s)
    random.seed(s)

    drowsy_labs = np.ones(drowsy_feats.shape[0])
    non_drowsy_labs = np.zeros(non_drowsy_feats.shape[0])

    X = np.vstack((drowsy_feats, non_drowsy_feats))
    y = np.concatenate((drowsy_labs, non_drowsy_labs))

    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.25, random_state=42)


Trois algorithmes de machine learning sont compar√©s :

1. SVM (Support Vector Machine).

.. code-block:: python

    svm = SVC(probability=True)
    svm.fit(X_train, y_train)
    svm_preds = svm.predict(X_test)
    svm_probas = svm.predict_proba(X_test)

2. MLP (Multi-Layer Perceptron).

.. code-block:: python

    mlp = MLPClassifier(hidden_layer_sizes=(5, 3), random_state=1, max_iter=1000)
    mlp.fit(X_train, y_train)
    mlp_preds = mlp.predict(X_test)
    mlp_probas = mlp.predict_proba(X_test)

3. Random Forest.

.. code-block:: python

    rf = RandomForestClassifier()
    rf.fit(X_train, y_train)
    rf_preds = rf.predict(X_test)
    rf_probas = rf.predict_proba(X_test)

D√©tection du Comportement de Fumer
==================================
preparation du modele CNN de fumee dans colab

1. *telecharger en ligne les data* :
   - importation du biblioth√®que n√©cessaire pour interagir avec Google Drive dans Google Colab.
   
.. code-block:: python

    from google.colab import drive
    drive.mount('/content/drive', force_remount=True)

   - telechargement de fichier kaggle.json pour telecharger dataset par collab apres creation d un dossier projet qui contient un dossier dataset et qui va contenir apres le modele  :
    
.. code-block:: python
     
     - # Load Data from Kaggle to directory
    from google.colab import files
    files.upload()

    !mkdir -p ~/.kaggle
    !cp kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
    !mkdir -p /content/drive/MyDrive/projet/dataset
    !kaggle datasets download -d sujaykapadnis/smoking -p /content/drive/MyDrive/projet/dataset
    !unzip -q /content/drive/MyDrive/projet/dataset/smoking.zip -d /content/drive/MyDrive/projet/dataset #extraire les dataset


√âvaluation et visualisation des Performances
============================================

pour fatigue 
------------

1. *√âvaluation des Performances* :
Pour √©valuer les performances des mod√®les de fatigue , les m√©triques suivantes sont calcul√©es :
   - Accuracy : Mesure globale des pr√©dictions correctes.
   - Precision : Pr√©cision des pr√©dictions positives.
   - Recall : Capacit√© √† d√©tecter les exemples positifs.
   - F1-score : Moyenne harmonique entre pr√©cision et rappel.

.. code-block:: python

   print("Classifier: RF")
   preds = rf_preds
   print(f"Accuracy: {accuracy_score(y_test, preds)}")
   print(f"Precision: {precision_score(y_test, preds)}")
   print(f"Macro Precision: {precision_score(y_test, preds, average='macro')}")
   print(f"Recall: {recall_score(y_test, preds)}")
   print(f"Macro F1 score: {f1_score(y_test, preds, average='macro')}")

Classifier: RF
Accuracy: 0.6812135132548569
Precision: 0.7006515231554851
Macro Precision: 0.6793614009907405
Recall: 0.7092691622103386
Macro F1 score: 0.6791399140903065
 
.. code-block:: python

    print("Classifier: MLP")
    preds = mlp_preds
    print(f"Accuracy: {accuracy_score(y_test, preds)}")
    print(f"Precision: {precision_score(y_test, preds)}")
    print(f"Macro Precision: {precision_score(y_test, preds, average='macro')}")
    print(f"Recall: {recall_score(y_test, preds)}")
    print(f"Macro F1 score: {f1_score(y_test, preds, average='macro')}")

Classifier: MLP
Accuracy: 0.6342233706574791
Precision: 0.7178362573099415
Macro Precision: 0.6489890506407863
Recall: 0.5251336898395722
Macro F1 score: 0.632404526982427

.. code-block:: python

    print("Classifier: SVM")
    preds = svm_preds
    print(f"Accuracy: {accuracy_score(y_test, preds)}")
    print(f"Precision: {precision_score(y_test, preds)}")
    print(f"Macro Precision: {precision_score(y_test, preds, average='macro')}")
    print(f"Recall: {recall_score(y_test, preds)}")
    print(f"Macro F1 score: {f1_score(y_test, preds, average='macro')}")

print("Classifier: SVM")
preds = svm_preds
print(f"Accuracy: {accuracy_score(y_test, preds)}")
print(f"Precision: {precision_score(y_test, preds)}")
print(f"Macro Precision: {precision_score(y_test, preds, average='macro')}")
print(f"Recall: {recall_score(y_test, preds)}")
print(f"Macro F1 score: {f1_score(y_test, preds, average='macro')}")


2. *Visualisation des R√©sultats* :

Les visualisations incluent :
   - Courbes ROC : Repr√©sentent le compromis entre le rappel et le taux de faux positifs.
   - Courbes Precision-Recall : Mettent en √©vidence les performances globales.

.. code-block:: python

    plt.figure(figsize=(8, 6))
    plt.title("ROC Curve for the models")
    # mlp
    fpr, tpr, _ = roc_curve(y_test, mlp_probas[:, 1])
    auc = round(roc_auc_score(y_test, mlp_probas[:, 1]), 4)
    plt.plot(fpr, tpr, label="MLP, AUC="+str(auc))

    # svm
    fpr, tpr, _ = roc_curve(y_test, svm_probas[:, 1])
    auc = round(roc_auc_score(y_test, svm_probas[:, 1]), 4)
    plt.plot(fpr, tpr, label="SVM, AUC="+str(auc))

    # RF
    fpr, tpr, _ = roc_curve(y_test, rf_probas[:, 1])
    auc = round(roc_auc_score(y_test, rf_probas[:, 1]), 4)
    plt.plot(fpr, tpr, label="RF, AUC="+str(auc))

    plt.plot(fpr, fpr, '--', label="No skill")
    plt.legend()
    plt.xlabel('True Positive Rate (TPR)')
    plt.ylabel('False Positive Rate (FPR)')
    plt.show()

.. image:: /image/1.png
   :alt: Texte alternatif pour l'image
   :width: 400px
   :align: center

.. code-block:: python

    plt.figure(figsize=(8, 6))
    plt.title("Precision-Recall Curve for the models")

    # mlp
    y, x, _ = precision_recall_curve(y_test, mlp_probas[:, 1])
    plt.plot(x, y, label="MLP")

    # svm
    y, x, _ = precision_recall_curve(y_test, svm_probas[:, 1])
    plt.plot(x, y, label="SVM")

    # RF
    y, x, _ = precision_recall_curve(y_test, rf_probas[:, 1])
    plt.plot(x, y, label="RF")

    plt.legend()
    plt.xlabel('Precision')
    plt.ylabel('Recall')
    plt.show()

.. image:: /image/2.png
   :alt: Texte alternatif pour l'image
   :width: 400px
   :align: center


.. code-block:: python

    import matplotlib.pyplot as plt
    from sklearn.metrics import precision_recall_curve
    import numpy as np

    def main():
        # Simuler des donn√©es fictives pour y_test et les probabilit√©s des mod√®les
        np.random.seed(42)
        y_test = np.random.randint(0, 2, 100)  # Labels binaires
        mlp_probas = np.random.rand(100, 2)    # Probabilit√©s du mod√®le MLP
        svm_probas = np.random.rand(100, 2)    # Probabilit√©s du mod√®le SVM
        rf_probas = np.random.rand(100, 2)     # Probabilit√©s du mod√®le RF

        # Tracer la courbe Precision-Recall
        plt.figure(figsize=(8, 6))
        plt.title("Precision-Recall Curve for the models")

        # MLP
        y, x, _ = precision_recall_curve(y_test, mlp_probas[:, 1])
        plt.plot(x, y, label="MLP")

        # SVM
        y, x, _ = precision_recall_curve(y_test, svm_probas[:, 1])
        plt.plot(x, y, label="SVM")

        # RF
        y, x, _ = precision_recall_curve(y_test, rf_probas[:, 1])
        plt.plot(x, y, label="RF")

        # Ajout des l√©gendes et labels
        plt.legend()
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.show()

    if _name_ == "_main_":
        main()

.. image:: /image/3.png
   :alt: Texte alternatif pour l'image
   :width: 400px
   :align: center

test des models de fatigue 
==========================

Cr√©er un r√©pertoire pour sauvegarder les mod√®les
------------------------------------------------

.. code-block:: python

    import os
    os.makedirs("./models", exist_ok=True)

    # Sauvegarder le mod√®le Random Forest
    with open("./models/rf_model.pkl", "wb") as rf_file:
    pickle.dump(rf, rf_file)

    # Sauvegarder le mod√®le SVM
    with open("./models/svm_model.pkl", "wb") as svm_file:
    pickle.dump(svm, svm_file)

    # Sauvegarder le mod√®le MLP
    with open("./models/mlp_model.pkl", "wb") as mlp_file:
    pickle.dump(mlp, mlp_file)

    print("Mod√®les sauvegard√©s avec succ√®s dans le dossier './models'.")


test des modeles  de Fatigue (rf , svm, mlp)
-------------------------------------------

Le code ci-dessous utilise OpenCV, MediaPipe et un mod√®le SVM pour d√©tecter la fatigue en surveillant les expressions faciales, telles que les mouvements des yeux et de la bouche, dans un flux vid√©o en temps r√©el. Si la fatigue est d√©tect√©e, une alerte sonore est d√©clench√©e.
pour changer le modele il faut juste remplacer svm par rf ou mlp

.. code-block:: python

    import cv2
    import mediapipe as mp
    import numpy as np
    import pygame
    import pickle
    import time

    # Charger les mod√®les entra√Æn√©s
    with open("./feats/phot_mp_drowsy_feats.pkl", "rb") as fp:
        drowsy_feats = pickle.load(fp)
    with open("./feats/phot_mp_not_drowsy_feats.pkl", "rb") as fp:
        non_drowsy_feats = pickle.load(fp)
    # Charger le mod√®le SVM
    with open("./models/svm_model.pkl", "rb") as svm_file:
        loaded_svm = pickle.load(svm_file)

    print("Mod√®le charg√© avec succ√®s.")

    # Initialisation des biblioth√®ques
    pygame.init()
    pygame.mixer.init()
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.3, min_tracking_confidence=0.8)
    mp_drawing = mp.solutions.drawing_utils

    # Sp√©cifications pour les points
    right_eye = [[33, 133], [160, 144], [159, 145], [158, 153]]  # right eye
    left_eye = [[263, 362], [387, 373], [386, 374], [385, 380]]  # left eye
    mouth = [[61, 291], [39, 181], [0, 17], [269, 405]]  # mouth

    # Fonction de calcul des distances
    def distance(p1, p2):
        return np.sqrt(np.sum((p1[:2] - p2[:2])**2))

    # Calcul EAR (Eye Aspect Ratio)
    def eye_aspect_ratio(landmarks, eye):
        N1 = distance(landmarks[eye[1][0]], landmarks[eye[1][1]])
        N2 = distance(landmarks[eye[2][0]], landmarks[eye[2][1]])
        N3 = distance(landmarks[eye[3][0]], landmarks[eye[3][1]])
        D = distance(landmarks[eye[0][0]], landmarks[eye[0][1]])
        return (N1 + N2 + N3) / (3 * D)

    # Calcul MAR (Mouth Aspect Ratio)
    def mouth_feature(landmarks):
        N1 = distance(landmarks[mouth[1][0]], landmarks[mouth[1][1]])
        N2 = distance(landmarks[mouth[2][0]], landmarks[mouth[2][1]])
        N3 = distance(landmarks[mouth[3][0]], landmarks[mouth[3][1]])
        D = distance(landmarks[mouth[0][0]], landmarks[mouth[0][1]])
        return (N1 + N2 + N3) / (3 * D)

    # Charger l'alerte sonore
    alert_sound = r"C:\Users\n\Desktop\projet ia\alert.mp3"
    pygame.mixer.music.load(alert_sound)

    # Capturer le flux vid√©o
    cap = cv2.VideoCapture(0)

    # Variables pour le timer
    fatigue_start_time = None  # Temps o√π la fatigue commence √† √™tre d√©tect√©e
    fatigue_threshold = 3  # Temps en secondes avant d√©clenchement de l'alarme

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Pr√©parer l'image pour MediaPipe
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        results = face_mesh.process(image)

        # Dessiner les r√©sultats
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                landmarks_positions = []
                for data_point in face_landmarks.landmark:
                    landmarks_positions.append([data_point.x, data_point.y, data_point.z])
                landmarks_positions = np.array(landmarks_positions)
                landmarks_positions[:, 0] *= frame.shape[1]
                landmarks_positions[:, 1] *= frame.shape[0]

                # Calculer EAR et MAR
                ear = (eye_aspect_ratio(landmarks_positions, left_eye) +
                       eye_aspect_ratio(landmarks_positions, right_eye)) / 2
                mar = mouth_feature(landmarks_positions)
                features = np.array([[ear, mar]])

                # Pr√©diction avec le mod√®le SVM
                pred = loaded_svm.predict(features)[0]

                # Gestion du timer pour la fatigue
                current_time = time.time()
                if pred == 1:  # Fatigue d√©tect√©e
                    if fatigue_start_time is None:
                        fatigue_start_time = current_time  # D√©marrer le timer
                    elif current_time - fatigue_start_time >= fatigue_threshold:
                        cv2.putText(image, "Fatigue detected!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                        if not pygame.mixer.music.get_busy():
                            pygame.mixer.music.play()
                else:
                    fatigue_start_time = None  # R√©initialiser si la fatigue n'est plus d√©tect√©e

                # Affichage du statut
                if fatigue_start_time is None:
                    cv2.putText(image, "Normal", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Afficher l'image
        cv2.imshow("Fatigue Detection", image)

        # Quitter avec la touche 'q'
        if cv2.waitKey(5) & 0xFF == ord('q'):
            break

    # Lib√©rer les ressources
    cap.release()
    cv2.destroyAllWindows()
    pygame.mixer.quit()

creation de l'application streamlit  
===================================

La g√©n√©ration d'une application Streamlit (par un fichier python app.py ) qui effectue la d√©tection de la fatigue par MAR, EAR et la fum√©e en temps r√©el. Lorsqu'un de ces signes est d√©tect√©, l'application √©met des alertes sonores

.. code-block:: python

    import streamlit as st
    import cv2
    import mediapipe as mp
    import numpy as np
    import pygame
    import pickle
    import time

    # Charger les mod√®les entra√Æn√©s
    with open("./feats/phot_mp_drowsy_feats.pkl", "rb") as fp:
        drowsy_feats = pickle.load(fp)
    with open("./feats/phot_mp_not_drowsy_feats.pkl", "rb") as fp:
        non_drowsy_feats = pickle.load(fp)
    with open("./models/svm_model.pkl", "rb") as svm_file:
        loaded_svm = pickle.load(svm_file)

    # Initialisation des biblioth√®ques
    pygame.init()
    pygame.mixer.init()
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.3, min_tracking_confidence=0.8)

    # Sp√©cifications pour les points
    right_eye = [[33, 133], [160, 144], [159, 145], [158, 153]]
    left_eye = [[263, 362], [387, 373], [386, 374], [385, 380]]
    mouth = [[61, 291], [39, 181], [0, 17], [269, 405]]

    # Fonction de calcul des distances
    def distance(p1, p2):
        return np.sqrt(np.sum((p1[:2] - p2[:2])**2))

    # Calcul EAR (Eye Aspect Ratio)
    def eye_aspect_ratio(landmarks, eye):
        N1 = distance(landmarks[eye[1][0]], landmarks[eye[1][1]])
        N2 = distance(landmarks[eye[2][0]], landmarks[eye[2][1]])
        N3 = distance(landmarks[eye[3][0]], landmarks[eye[3][1]])
        D = distance(landmarks[eye[0][0]], landmarks[eye[0][1]])
        return (N1 + N2 + N3) / (3 * D)

    # Calcul MAR (Mouth Aspect Ratio)
    def mouth_feature(landmarks):
        N1 = distance(landmarks[mouth[1][0]], landmarks[mouth[1][1]])
        N2 = distance(landmarks[mouth[2][0]], landmarks[mouth[2][1]])
        N3 = distance(landmarks[mouth[3][0]], landmarks[mouth[3][1]])
        D = distance(landmarks[mouth[0][0]], landmarks[mouth[0][1]])
        return (N1 + N2 + N3) / (3 * D)

    # Charger l'alerte sonore
    alert_sound = r"C:\Users\n\Desktop\projet ia\alert.mp3"
    pygame.mixer.music.load(alert_sound)

    # D√©finir l'application Streamlit
    st.set_page_config(page_title="D√©tection de Fatigue", layout="wide", initial_sidebar_state="expanded")

    st.title("üõå D√©tection de Fatigue en Temps R√©el")
    st.write("""
    Cette application utilise *MediaPipe* et un mod√®le SVM pr√©-entra√Æn√© pour d√©tecter les signes de fatigue 
    en temps r√©el. Les alertes sonores sont d√©clench√©es lorsqu'une fatigue prolong√©e est d√©tect√©e.
    """)

    run = st.checkbox("Activer la d√©tection de fatigue")
    fatigue_threshold = st.slider("Seuil d'alerte (secondes)", 1, 10, 3)

    if run:
        # Capturer le flux vid√©o
        cap = cv2.VideoCapture(0)
        fatigue_start_time = None

        stframe = st.empty()

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                st.warning("Impossible d'acc√©der √† la cam√©ra.")
                break

            # Pr√©parer l'image pour MediaPipe
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(image)

            if results.multi_face_landmarks:
                for face_landmarks in results.multi_face_landmarks:
                    landmarks_positions = []
                    for data_point in face_landmarks.landmark:
                        landmarks_positions.append([data_point.x, data_point.y, data_point.z])
                    landmarks_positions = np.array(landmarks_positions)
                    landmarks_positions[:, 0] *= frame.shape[1]
                    landmarks_positions[:, 1] *= frame.shape[0]

                    # Calculer EAR et MAR
                    ear = (eye_aspect_ratio(landmarks_positions, left_eye) +
                        eye_aspect_ratio(landmarks_positions, right_eye)) / 2
                    mar = mouth_feature(landmarks_positions)
                    features = np.array([[ear, mar]])

                    # Pr√©diction avec le mod√®le SVM
                    pred = loaded_svm.predict(features)[0]
                    current_time = time.time()

                    # Gestion du timer pour la fatigue
                    if pred == 1:  # Fatigue d√©tect√©e
                        if fatigue_start_time is None:
                            fatigue_start_time = current_time
                        elif current_time - fatigue_start_time >= fatigue_threshold:
                            if not pygame.mixer.music.get_busy():
                                pygame.mixer.music.play()
                            cv2.putText(image, "Fatigue d√©tect√©e!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                    else:
                        fatigue_start_time = None

            # Convertir pour Streamlit
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            stframe.image(frame, channels="RGB", use_column_width=True)

        cap.release() 

pour l'execution de cette application il faut taper en terminal streamlit run app.py



Travaux Futurs
==============

1. Am√©liorer les mod√®les en utilisant plus de donn√©es.
2. √âtendre la classification pour inclure d'autres comportements (vapoter, boire, etc.).

Conclusion
==========

Ce projet d√©montre la puissance de *MediaPipe* et *TensorFlow* pour r√©soudre des probl√®mes critiques li√©s √† la s√©curit√© et au bien-√™tre. L'int√©gration de ces outils offre une solution robuste¬†et¬†extensible.
